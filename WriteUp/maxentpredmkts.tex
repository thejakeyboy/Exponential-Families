% Adjusted to add \usepackage[numbers]{natbib}
% \bibliographystyle{acmsmall} and \documentclass[prodmode,acmec]{ec-acmsmall}
% Jan 5, 2013 - David Parkes
%
% v2-acmsmall-sample.tex, dated March 6 2012
% This is a sample file for ACM small trim journals
%
% Compilation using 'acmsmall.cls' - version 1.3 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - March 2012

\documentclass[prodmode,acmec]{ec-acmsmall} % Aptara syntax

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Metadata Information
\acmVolume{9}
\acmNumber{4}
\acmArticle{39}
\acmYear{2013}
\acmMonth{6}
\usepackage[numbers]{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{amsthm}
\usepackage{parskip}
\usepackage{booktabs}

%new commands
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\mX}{\mathcal{X}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\grad}{\nabla}
\newcommand{\Exp}{\mathbf{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\q}{\mathbf{q}}
\newcommand{\mM}{\ensuremath{\mathcal{M}}}
\newcommand{\hmu}{\ensuremath{\hat{\mu}}}
\newcommand{\mP}{\ensuremath{\mathcal{P}}}
\newcommand{\mY}{\ensuremath{\mathcal{Y}}}
\newcommand{\mS}{\ensuremath{\mathcal{S}}}
\newcommand{\ds}{\displaystyle}
\newcommand{\mF}{\mathcal{F}}
\newcommand{\mD}{\mathcal{D}}
\newcommand{\htheta}{\hat{\theta}}
%---------------------------------------------------------------------------
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{con}[thm]{Conjecture}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{ex}[thm]{Example}
\newtheorem{qn}[thm]{Question}
\newcommand{\defn}{\bigbreak\noindent{\bf Definition. }}
\newcommand{\undefn}{\bigbreak}
\newcommand{\rmk}{\bigbreak\noindent{\bf Remark. }}
\newcommand{\unrmk}{\bigbreak}
%\newcommand{\proof}{\noindent{\bf Proof. }}
\newcommand{\unproof}{\hfill$\Box$\bigbreak}
\newcommand{\defeq}{\stackrel{\mathrm{def}}{=}}
\newcommand{\holds}[1]{\stackrel{?}{#1}}
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\ie} {{\it i.e. }}
\newcommand{\eg} {{\it e.g. }}
\newcommand{\etal} {{\it et al. }}
\newcommand{\E}{\mathbf{E}}
\newcommand{\betavec}{\pmb{\beta}}
\newcommand{\qvec}{\mathbf{q}}
\newcommand{\lpf}{\mathbf{\psi}} %logpartition function
\newcommand{\family}{\mathcal F} 
\newcommand{\suff}{\pmb{\phi}} % suff stats
\newcommand{\muvec}{\pmb{\mu}}
\newcommand{\st}{\hspace{10pt}\mathrm{s.t.}\hspace{10pt}}
\newcommand{\norm}[1]{|| #1 ||}
\newcommand{\tp}{\tilde{p}}


% Document starts

\begin{document}
% Page heads
\markboth{Abernethy et al.}{Maximum Entropy Prediction Markets}
% Title portion
\title{Maximum Entropy Prediction Markets}
\author{JACOB ABERNETHY
\affil{University of Michigan, Ann Arbor}
SINDHU KUTTY
\affil{University of Michigan, Ann Arbor}
S\'{E}BASTIEN LAHAIE
\affil{Microsoft Research, New York City}
RAHUL SAMI
\affil{University of Michigan, Ann Arbor}}
%%\author{GANG ZHOU
%%\affil{College of William and Mary}
%%YAFENG WU
%%\affil{University of Virginia}
%%TING YAN
%%\affil{Eaton Innovation Center}
%%TIAN HE
%%\affil{University of Minnesota}
%%CHENGDU HUANG
%%\affil{Google}
%%JOHN A. STANKOVIC
%%\affil{University of Virginia}
%%TAREK F. ABDELZAHER
%%\affil{University of Illinois at Urbana-Champaign}}
% NOTE! Affiliations placed here should be for the institution where the
%       BULK of the research was done. If the author has gone to a new
%       institution, before publication, the (above) affiliation should NOT be changed.
%       The authors 'current' address may be given in the "Author's addresses:" block (below).
%       So for example, Mr. Abdelzaher, the bulk of the research was done at UIUC, and he is
%       currently affiliated with NASA.

\begin{abstract}
In this paper, we draw connections between the aggregation performed by learning algorithms and the information aggregation done in prediction markets. We show that, under reasonable conditions, the behavior of rational traders can be understood as the result of performing a learning algorithm on their private data. Similarly,  the market state can be interpreted as a distribution over the outcome space. In particular, we show that a proper scoring rule can be derived from maximum entropy distributions. This scoring rule can be used as a general form of LMSR in prediction markets with over continuous outcome spaces. In order to provide insight on the behavior of rational traders in the market, we use the concept of exponential utility. We show that the traders' behavior can be understood as updating his belief using a Bayesian process and updating the market state in accordance with this utility function. These maxent prediction  markets can also be used to design  markets that are robust against adversarial traders. In fact, when traders are required to report their budgets and their beliefs, we can show that an informative trader eventually makes money and damaging traders eventually have limited influence in the market. Using ideas from convex analysis and the properties of the prediction market, we analyze the properties of the maxent market maker thus providing insight into the information content of the prediction market.
\end{abstract}

\category{J.4}{Social and Behavioral Sciences}{Economics}
\category{I.2.6} {Artificial Intelligence}{Learning}

\terms{Prediction Market Design, Machine Learning Algorithms}

\keywords{Exponential Families, Bayesian Learning, Maximum Entropy Distributions, generalized LMSR}

\acmformat{Jacob Abernethy, Sindhu Kutty, S\'{e}bastien Lahaie, and Rahul Sami, 2014. Maximum Entropy Prediction Markets}
%%Gang Zhou, Yafeng Wu, Ting Yan, Tian He, Chengdu Huang, John A. Stankovic, and Tarek F. Abdelzaher, 2010. A multifrequency MAC specially designed for  wireless sensor network applications.

% At a minimum you need to supply the author names, year and a title.
% IMPORTANT:
% Full first names whenever they are known, surname last, followed by a period.
% In the case of two authors, 'and' is placed between them.
% In the case of three or more authors, the serial comma is used, that is, all author names
% except the last one but including the penultimate author's name are followed by a comma,
% and then 'and' is placed before the final author's name.
% If only first and middle initials are known, then each initial
% is followed by a period and they are separated by a space.
% The remaining information (journal title, volume, article number, date, etc.) is 'auto-generated'.

\begin{bottomstuff}
%%This work is supported by the National Science Foundation, under
%%grant CNS-0435060, grant CCR-0325197 and grant EN-CS-0329609.
%%
%%Author's addresses: G. Zhou, Computer Science Department,
%%College of William and Mary; Y. Wu  {and} J. A. Stankovic,
%%Computer Science Department, University of Virginia; T. Yan,
%%Eaton Innovation Center; T. He, Computer Science Department,
%%University of Minnesota; C. Huang, Google; T. F. Abdelzaher,
%%(Current address) NASA Ames Research Center, Moffett Field, California 94035.
\end{bottomstuff}

\maketitle


\section{Introduction}
%------------------------------------------------------------
%Outline
%------------------------------------------------------------
%-introduction [Sindhu; Thursday]
%-literature review
%- maxent scoring rule [Sebastien]
%	- generalized lmsr
%	- exp family review
%	[- does modularity (a la Hanson) still hold?]
%- prediction market setup [Sebastien]
%	- MLE with risk neutrality
% 	need a definition of term 'exponential family prediction market'
%- conjugate prior market [Sindhu; Wednesday]
%	- also on hyperparam?
%- exp-util [Sebastien]
%- equilibrium for repeated trades [Sindhu; Tuesday] done
% 	[- rate of convergence?]
%- budgets and adversarial markets [Sindhu; Monday] done
% [any liquidity results?] [Sindhu: weekend]
% conclusion
% bibliography

%------------------------------------------------------------
% Naming conventions:
%------------------------------------------------------------
%htheta belief
%theta current market state
%thetaÕ final state
%T is the log partition function


We have dual goals in this paper. On the one hand, we highlight a structural similarity between prediction markets and exponential families. We see this as the syntax of our prediction market mechanism. On the other, this formulation has rich semantics as well: it allows for analysis of market behavior under various environments. For instance, we analyze market behavior with budget limited traders, traders that are exponential-utility-maximizers.
%\section{Notation and Definitions}
%convex conjugates, bregman divergences, scoring rules, sufficient statitics, exp families, Bayesian updates, conjugate prior.




\pagebreak

\section{Generalized Log Scoring Rules}


We consider a measurable space that consists of a set of outcomes $\mX$ together with a $\sigma$-algebra $\mF$. An agent or expert has a \emph{belief} over potential outcomes taking the form of a probability measure absolutely continuous with respect to a base measure~$\nu$.\footnote{Recall that a measure $P$ is absolutely continuous with respect to~$\nu$ if $P(A) = 0$ for every $A \in \mF$ for which $\nu(A) = 0$. In essence the base measure~$\nu$ restricts the support of $P$. In our examples $\nu$ will typically be a restriction of the Lebesgue measure for continuous outcomes or the counting measure for discrete outcomes.} Throughout we represent the belief as the corresponding density $p$ with respect to~$\nu$. Let $\mP$ denote the set of all such probability densities, and let $\mD \subseteq \mP$ be a subset from which the beliefs are drawn.

We are interested in eliciting information about the agent's belief, in particular expectation information. Let $\phi: \mX \rightarrow \bR^d$ be a vector-valued random variable or \emph{statistic}, where $d$ is finite. The aim is to elicit $\mu = \E_{p}[\phi(x)]$ where $x$ is the random outcome. A \emph{scoring rule} is a device for this purpose. Let $$\mM = \left\{ \mu \in \bR^d : \Exp_p[\phi(x)] = \mu,\, \mbox{for some $p \in \mP$} \right\}$$ be the set of realizable statistic expectations.  A scoring rule $S : \mM \times \mX \rightarrow \bR \cup \{-\infty\}$ pays the agent $S(\hmu,x)$ according to how well its report $\hmu \in \mM$ agrees with the eventual outcome $x \in \mX$. 
%
\begin{definition}
A scoring rule $S$ is  \emph{proper over domain $\mD$} for statistic $\phi$ if for each $\mu \in \mM$ and $p \in \mD$ with expected statistic $\mu$, we have
%
\begin{equation} \label{eq-proper}
\Exp_p[S(\mu,x)] \geq \Exp_p[S(\hmu,x)]
\end{equation}
%
for all alternative $\hmu$. If the domain is $\mD = \mP$, the set of all possible densities, then we simply say the scoring rule is \emph{proper}.
\end{definition}
%

\noindent
Note that given a proper scoring rule $S$ any affine transformation $\tilde{S}(\mu,x) = aS(\mu,x) + b(x)$ of the rule, with $a > 0$ and $b$ an arbitrary real-valued function of the outcomes, again yields a proper scoring rule termed \emph{equivalent}. Throughout we will often apply such affine transformations to obtain the clearest version of the scoring rule. We will also focus on scoring rules where inequality~(\ref{eq-proper}) is strict to avoid trivial cases such as constant scoring rules.

Our definition is more general than classical scoring rules in two respects. Classically, scoring rules take in the entire density $p$ rather than just some statistic, and incentive compatibility must hold over all of $\mP$. When the outcome space is large or infinite, it is not feasible to directly communicate $p$, so our definition allows for summary information of the belief. If there is a bijection between the sets $\mD$ and $\mM$, then we say that $\mM$ parametrizes $\mD$ and write $p(\cdot\,;\mu)$ for the density mapping to $\mu$.

Note that this definition of a scoring rule places only mild information requirements on the part of the agent to ensure truthful reporting. Because condition~(\ref{eq-proper}) holds for all $p$ consistent with expectation $\mu$, it is enough for the agent to simply know the latter and not the complete density to be properly incentivized. However, the agent must also agree that the density is actually drawn from domain $\mD$; if the scoring rule is proper, it is enough that the agent agree with the support of the density as implicitly defined by base measure $\nu$. 

When the outcome space is finite we can recover classical scoring rules from the definition by taking $\mD = \mP$ and using the statistic $\phi : \mX \rightarrow \{0,1\}^{\mX}$ that maps an outcome $x$ to a unit vector with a 1 in the component corresponding to $x$. The expectation of $\phi$ is then exactly the probability mass function.

\subsection{Proper Scoring from Consistency}

Our starting point for designing proper scoring rules is the classic logarithmic scoring rule for eliciting probabilities in the case of finite outcomes. This rule is simply $S(p,x) = \log p(x)$ and there are several ways to understand why it is proper, with different implications for generalization. 

First observe that the scoring rule compensates the agent with the log likelihood it assigned to the outcome, so the agent is maximizing the expected log likelihood via its report. Now it is a fundamental result in statistics that, under a variety of different sufficient conditions, the maximum likelihood estimator is statistically consistent; see~\cite{} for an overview of such conditions. This means that in the limit (as the size of the i.i.d.\ empirical sample increases) the log likelihood is maximized by the true parameters. By this reasoning, the following result follows immediately from the definition of consistency and the law of large numbers.
%
\begin{theorem} \label{max-like}
Suppose that $\mM$ parametrizes $\mD$. The logarithmic scoring rule defined by
%
\begin{equation} \label{log-score}
S(\mu, x) = \log p(x;\mu),
\end{equation}
%
where $\mu \in \mM$, is proper over $\mD$ if and only if the maximum likelihood estimator for $\mu$ is consistent.
\end{theorem}
%
We note that although our focus is on parametrizations by expectations, the preceding theorem in fact holds for more generic parameter spaces. The range of scoring rules it provides is broader than those characterized by~\cite{}, because it allows for a restriction over the set of beliefs. On the other hand, its applicability is limited to those domains $\mD$ that can be parametrized by the relevant statistics. The following provide concrete examples of these points. 
%
\begin{example}
Suppose the outcomes are supported on $[1,+\infty)$ and follow a Pareto distribution with density $f(x;\alpha) = \alpha/x^{\alpha+1}$ parametrized by an index $\alpha > 0$. The mean $\mu$ is related to the index via the one-to-one mapping $\mu = \frac{\alpha}{\alpha-1}$, so the density can alternatively be parametrized by the mean. Theorem~\ref{max-like} gives the following proper scoring rule for the mean of a Pareto distribution with support on $[1,+\infty)$:
%
\begin{equation} \label{pareto-scoring}
S(\mu,x) = \log \frac{\mu}{\mu-1} - \left(\frac{\mu}{\mu-1} + 1\right) \log x.
\end{equation}
%
We stress that the rule can only elicit the mean assuming the agent knows its belief is a Pareto distribution over $[1,+\infty]$. It does not elicit the mean of other families of densities parametrized by the mean (e.g., the exponential distribution). 
\end{example}
%
%
\begin{example}
Suppose the outcomes are supported on $(-\infty,+\infty)$ and follow a Cauchy distribution with density $f(x;m) = 1/\pi[1+(x-m)^2]$, parametrized by the median $m$. Theorem~\ref{max-like} leads to the following proper scoring rule for the median:
%
\begin{equation} \label{cauchy-scoring}
S(m,x) = -\log[1+(x-m)^2].
\end{equation}
%
Note that the median cannot be obtained as the expectation of any statistic. This highlights the range of parameters than may be elicited by the log score in different circumstances.
\end{example}
%

\subsection{Proper Scoring from Maximum Entropy}

We now turn to scoring rules that are proper over the entire domain of densities $\mP$. Our construction is the same as in the previous section: we take the log likelihood of a density parametrized by the relevant statistics. If the family of densities is chosen appropriately, the resulting rule will in fact hold over all $\mP$. To achieve this we draw on a well-known duality between maximum likelihood and maximum entropy.

\subsubsection*{Exponential Families}

We let $p(x;\mu)$ be the maximum entropy distribution with expected statistic $\mu$. Specifically, it is the solution to the following mathematical program:\footnote{We assume throughout that the minimum is finite and achieved for all $\mu \in \mM$. Some care is needed to ensure this holds for specific statistics and outcome spaces. For example, taking outcomes to be the real numbers, there is no maximum entropy distribution with a given mean $\mu$ (one can take densities tending towards the uniform distribution over the reals), but there is always a solution if we constrain both the mean and variance.}
%
\begin{equation} \label{maxent-prog}
 \min_{p \in \mP} F(p) \st \Exp_{p}[\phi(x)] = \mu,
\end{equation}
%
where the objective function is the negative entropy of the distribution, namely
%
\[ F(p) = \int_{x \in \mX} p(x) \log p(x)\, d\nu(x).
\]
%
Note that the explicit set of constraints in~(\ref{maxent-prog}) are linear, and to stress this fact we find it helpful to re-write them as $A_{\phi} p = \mu$, where $A_{\phi}$ is the expectation operator of statistics $\phi$. We let $G : \mM \rightarrow \bR$ be the optimal value function of~(\ref{maxent-prog}), meaning $G(\mu)$ is the (negative) entropy of the maximum entropy distribution with expected statistics $\mu$. 

It is well-known that solutions to~(\ref{maxent-prog}) are \emph{exponential family} distributions, whose densities with respect to $\nu$ take the form
%
\begin{equation} \label{exp-fam}
p(x;\theta) = \exp( \la \theta, \phi(x) \ra - T(\theta) ).
\end{equation}
%
The density is stated here in terms of its \emph{natural} parametrization $\theta \in \bR^d$, where $\theta$ arises as the Lagrange multiplier associated with linear constraints $A_{\phi}p = \mu$. The term $T(\theta)$ essentially arises as the multiplier for the normalization constraint (the density must integrate to 1), and so ensures that~(\ref{exp-fam}) is normalized:
%
\begin{equation} \label{log-part}
T(\theta) = \log \int_{\mX} \exp \la \theta, \phi(x) \ra \,d\nu(x).
\end{equation}
%
The function $T$ is known as the \emph{log-partition} or \emph{cumulant} function corresponding to the exponential family. Its domain is $\Theta = \{ \theta \in \bR^d : T(\theta) < +\infty \}$, called the natural parameter space. The exponential family is \emph{regular} if $\Theta$ is open---almost all exponential families of interest, and all those we consider in this work, are regular. The family is \emph{minimal} if there is no $\alpha \in \Theta$ such that $\la \alpha, \phi(x) \ra$ is a constant over $\mX$ ($\nu$-almost everywhere); minimality is a property of the associated statistic $\phi$, usually called the \emph{sufficient statistic} in the literature. 

The following proposition collects the relevant results on regular exponential families and their associated parameter domains and functions. A convex function $T$ is of \emph{Legendre type} if it is proper, closed, strictly convex and differentiable on the interior of its domain, and $\lim_{\theta \rightarrow \bar{\theta}} \norm{\grad T(\theta)} = +\infty$ when $\bar{\theta}$ lies on the boundary of the domain. 
%
\begin{prop}\label{prop:exp}
Consider a regular exponential family with minimal sufficient statistic. The following properties hold:
\begin{enumerate}
\item $T$ and $G$ are of Legendre type, and $T = G^*$ (equivalently $G = T^*$). 
\item The gradient map $\grad T$ is one-to-one and onto the interior of $\mM$. Its inverse is $\grad G$ which is one-to-one and onto the interior of $\Theta$.
\item The exponential family distribution with natural parameter $\theta \in \Theta$ has expected statistic $\mu = \Exp_p[\phi(x)] = \grad T(\theta)$.
\item The maximum entropy distribution with expected statistic $\mu$ is the exponential family distribution with natural parameter $\theta = \grad G(\mu)$. 
\end{enumerate}
\end{prop}
%
In the above $T^*$ denotes as usual the convex conjugate of $T$, which here can be evaluated as $T^*(\mu) = \sup_{\theta \in \Theta} \la \theta, \mu \ra - T(\theta)$. Similarly, $G^*(\theta) = \sup_{\mu \in \mM} \la \theta, \mu \ra - G(\mu)$.

\subsubsection*{Proper Log Scoring}

We are now in a position to analyze the log scoring rule under exponential family distributions. From our discussion so far, we have that an exponential family density can be parametrized either by the natural parameter $\theta$, or by the mean parameter $\mu$, and that the two are related by the invertible gradient map $\mu = \grad T(\theta)$. We will write $p(x;\theta)$ or $p(x;\mu)$ given the parametrization used, which should be clear from context.

The following observation is crucial. Let $\tp \in \mP$ be a density (not necessarily an exponential family) with expected statistic $\mu$, let $p(\cdot\,;\mu)$ be the exponential family with the same expected statistic, and let $\mu' \in \mM$ be an alternative report. Then note that
%
\begin{equation} \label{equalizer-rule}
\Exp_{\tp}[\log p(x;\mu')] = \Exp_{p(\cdot;\mu)}[\log p(x;\mu')] = \la \theta', \mu \ra - T(\theta'),
\end{equation}
%
where $\theta' = \grad G(\mu')$ is the natural parameter for the exponential family with statistic $\mu'$. We see from this that the expected log score only depends on the expectation $\mu$ of the underlying density, not the full density, so when reasoning about the agent's reporting incentives we can just as well assume its beliefs are from an exponential family for simplicity.
%
\begin{theorem} \label{maxent-score}
Consider the logarithmic scoring rule $S(\mu,x) = \log p(x;\mu)$ defined over a set of densities $\mD$ parametrized by $\mM$. The scoring rule is proper over the entire domain of densities $\mP$ if and only if $\mD$ is the exponential family with statistic $\phi$. 
\end{theorem}
%
%
\begin{proof}
Let $\mu,\mu' \in \mM$ be the agent's true belief and an alternative report, and let $p,p' \in \mP$ be densities consistent with each belief. Let $\theta = \grad G(\mu)$ and $\theta' = \grad G(\mu')$, and note that $\mu = \grad T(\theta)$. We have
%
\begin{eqnarray}
& & \Exp_p[\log p(x;\mu)] - \Exp_p[\log p(x;\mu')] \nonumber\\
& = & \la \theta, \mu \ra - T(\theta) - \la \theta', \mu \ra + T(\theta') \nonumber\\
& = & T(\theta') - T(\theta) - \la \theta' - \theta, \mu \ra \nonumber\\
& = & T(\theta') - T(\theta) - \la \theta' - \theta, \grad T(\theta) \ra. \label{breg-div}
\end{eqnarray}
%
The latter is positive by the strict convexity of $T$, which shows that the log score is proper. For the converse, assume the defined log score is proper. By the Savage characterization of proper scoring rules for expectations (see~\cite{}), we must have
%
$$
S(\mu,x) = G(\mu) - \la \grad G(\mu), \mu - \phi(x) \ra
$$
for some strictly convex function $G$. Let $T = G^*$, so that $\grad G = {\grad T}^{-1}$, and let $\theta = \grad G(\mu)$. Then the above takes the form
%
\begin{eqnarray*}
\log p(x;\mu) & = & G(\mu) - \la \grad G(\mu), \mu - \phi(x) \ra \\
& = & \la \theta, \mu \ra - T(\theta) - \la \theta, \mu - \phi(x) \ra \\
& = & \la \theta, \phi(x) \ra - T(\theta),
\end{eqnarray*}
%
which shows that $p(x;\mu)$ takes the form of an exponential family.
\end{proof}
%
As further intuition for the result, note that~(\ref{breg-div}) is the definition of the Bregman divergence with respect to strictly convex function $T$. Therefore we have 
%
\begin{equation*}
\Exp_p[\log p(x;\mu)] - \Exp_p[\log p(x;\mu')] = D_T(\theta',\theta) = D_G(\mu, \mu'),
\end{equation*}
%
where the last equality is a well-known identity relating the Bregman divergences of $T$ and $T^* = G$~\citep{}. The equation states that the agent's regret from misreporting its mean parameter does not depend on the full density $p$, only the mean $\mu$.   

Theorem~\ref{maxent-score} leads to a straightforward procedure for constructing score rules for expectations: define the relevant statistic, and consider the maximum entropy (equivalently, exponential family) distribution consistent with the agent's reported mean $\mu$. The agent is compensated according to the log likelihood of the eventual outcome according to the latter. The interpretation is that the agent is only providing partial information about the underlying density, so the principal first infers a full density according to the principle of maximum entropy, and then scores the agent using the usual log score.
  
An advantage of this generalization of the log score is that, for many statistics of interest, it leads to simple \emph{closed-form} formulas for the scoring rule. The following examples illustrate the construction.
%
\begin{example} \label{ex:exponential}
As base measure we take the Lebesgue restricted to $[0,+\infty)$, and we consider the statistic $\phi(x) = x$ so that we are simply eliciting the mean. The maximum entropy distribution with a given mean $\mu$ is the exponential distribution, and taking its log density gives the scoring rule
%
\begin{equation} \label{exp-scoring}
S(\mu, x) = -\frac{x}{\mu} - \log \mu.
\end{equation}
%
We stress that although this rule is derived from the exponential distribution, Theorem~\ref{maxent-score} implies that it elicits the mean of any distribution supported on the non-negative reals (e.g., Pareto, lognormal), in contrast to~(\ref{pareto-scoring}). Indeed, it is easy to see that the expected score~(\ref{exp-scoring}) depends only on the mean of the agent's belief because it is linear in $x$. 
%
Typically the exponential distribution is parametrized by its rate $\lambda = 1/\mu$. In that case the rule takes the form $S(\mu, x) = -\lambda x + \log \lambda$, and we see that $\lambda$ corresponds to the natural parameter.
\end{example}
%
%
\begin{example}
As a base measure we take the Lebesgue over the real numbers $\bR$. We are interested in eliciting the mean $\mu$ and variance $\sigma^2$, so as a statistic we take $\phi(x) = (x, x^2)$ for which $\Exp_p[\phi(x)] = (\mu, \mu^2+\sigma^2)$. The maximum entropy distribution for a given mean and variance is the normal distribution, and taking its log density gives the scoring rule
%
\begin{equation}
S((\mu, \sigma^2), x) = -\frac{(x-\mu)^2}{\sigma^2} - \log \sigma^2.
\end{equation}
%
Again, we stress that this scoring rule elicits the mean and variance of any density over the real numbers, not just those of a normal distribution. The construction easily generalizes to a multi-dimensional outcome space by taking the log density of the multivariate normal:
%
\begin{equation}
S(({\boldsymbol \mu}, {\boldsymbol \Sigma}), \boldsymbol{x}) = -( \boldsymbol{x} - {\boldsymbol \mu})'{\boldsymbol \Sigma}^{-1}( \boldsymbol{x} - {\boldsymbol \mu}) - \log |{\boldsymbol \Sigma}|.
\end{equation}
%
Here the statistics being elicited are the mean vector ${\boldsymbol \mu}$ and the covariance matrix ${\boldsymbol \Sigma}$. These scoring rules have been studied by~\citet{} as rules that only depend on the mean and variance of the reported density. They note that these rules are weakly proper (because they do not distinguish between densities with the same first and second moments), but do not make the point that knowledge of the full density is not necessary on the part of the agent.
\end{example}
%
 

 
%\subsection{Exponential Smoothing}
%One way to model the exponential family prediction market might be through exponential smoothing.
%\begin{itemize}
%\item A \emph{time series} is a set of data points sampled at periodic instances.
%\item \emph{Smoothing} is the process of creating an approximation function based on time series that while capturing  patterns in the data is relatively insensitive to noise.
%\item Natural first step is to use moving average. But average on how many samples (say $k$)? What to do until the first $k$ samples have been received?
%\item
%Next attempt might be to use weighted moving average. Advantage is that you can give higher weight to more recent terms. But still same disadvantage as simple moving average technique.
%\item
%\emph{Exponential Smoothing} attempts to remove this disadvantage.
%\end{itemize}
%
%Exponential smoothing is defined iteratively for $\alpha\in(0,1)$ as
%\begin{eqnarray*}
%s_{1} &=& x_{0}\\
%s_{t}&=&\alpha x_{t-1}+(1-\alpha)s_{t-1}
%\end{eqnarray*}
%
%Smaller values of $\alpha$ means greater smoothing e.g., $\alpha=0$ gives a constant function ($=x_{0}$).
%Note that the selection of $x_{0}$ is important -- and increases in importance as $\alpha$. Some advantages over moving average: all data points matter (decreasing importance with time) and computationally only last data point need be stored.






\vfill
\pagebreak
\section{Maximum Entropy Market Making}

In a single-agent setting, a scoring rule is used to \emph{elicit} the agent's belief. In a multi-agent setting, an information market can be used to \emph{aggregate} the beliefs of the agents. In his seminal paper~\citet{} introduced the idea of a market scoring rule, which inherits the appealing elicitation and aggregation properties of both in order to perform well in thin or thick markets. In this section, we adapt the generalized log scoring rule to a market scoring rule which leads to markets with simple closed-form cost functions for many statistics of interest. The exponential families framework also allows us to move beyond risk-neutral agents to understand the aggregation properties of information markets under alternative behaviors, such as Bayesian updating and risk aversion. 



\subsection{Information Market}

In an information market an agent's expected belief $\mu$ is elicited indirectly through the purchase and sale of contingent claim securities. Under this approach, each component~$i$ of the statistic $\phi$ is interpreted as the payoff function of a security; that is, a single share of security $i$ pays off $\phi_i(x)$ when $x \in \mX$ occurs. Thus if the portfolio of shares held by the agent is $\delta \in \bR^d$, where entry $\delta_i$ corresponds to the number of shares of security $i$, then the payoff to the agent when $x$ occurs is evaluated by taking the inner product $\la \delta, \phi(x) \ra$. 

As a concrete example, recall that in the classic finite-outcome case the statistic has a component for each outcome $x$ such that $\phi_x(x') = 1$ if $x' = x$ and 0 otherwise. Therefore the corresponding security pays 1 dollar if outcome $x$ occurs. (These are known as Arrow-Debreu securities.) In Example~\ref{ex:exponential} the one-dimensional statistic is $\phi(x) = x$, corresponding to a security whose payoff is linear in the outcome $x \in \bR_+$. (This amounts to a futures contract.) 

The standard way to implement an information market in the literature, due to~\citet{}, is via a centralized market maker. The market maker maintains a convex, differentiable cost function $C : \bR^d \rightarrow (-\infty,+\infty]$, where $C(\theta)$ records the revenue collected when the vector of outstanding shares is $\theta$. The cost to an agent of purchasing portfolio $\delta$ under a market state of $\theta$ is $C(\theta + \delta) - C(\theta)$, and therefore the instantaneous prices of the securities are given by the gradient $\grad C(\theta)$. 

A risk-neutral agent will choose to acquire shares up to the point where, for each share, expected payoff equals marginal price. Formally, if the agent acquires portfolio $\delta$, then we must have
%
\begin{equation} \label{risk-neutral-agent}
\Exp_p[\phi(x)] = \grad C(\theta + \delta).
\end{equation}
%
In this way, by its choice of $\delta$, the agent reveals that its expected belief is $\mu = \grad C(\theta + \delta)$. We stress that this observation relies on the assumptions that 1) the agent is risk-neutral, 2) the agent does not incorporate the market's information into its own beliefs, and 3) the agent is not budget constrained.

\subsection{Information-Theoretic Interpretation}



In the remainder of this paper we focus on the following cost function, which arises from the ``generalized'' logarithmic market scoring rule (LMSR):
%
\begin{equation} \label{lmsr-cost}
C(\theta) = \log \int_{\mX} \exp \la \theta, \phi(x) \ra \,d\nu(x).
\end{equation}
%
This is of course exactly the log-partition function~(\ref{log-part}) for the exponential family with sufficient statistic $\phi$, and we recover the classic LMSR using outcome indicator vectors as statistics. Because an agent would never select a portfolio with infinite cost, the effective domain (i.e., the possible vectors of outstanding shares) of $C$ is $\Theta = \{\theta \in \bR^d : C(\theta) < +\infty \}$.




Observe that in the context of our earlier multinomial example, cost function~(\ref{cost}) is exactly the cost function for Hanson's logarithmic market scoring rule.

. In the context of the multinomial distribution example, $\Omega = \bR^d$, so that shares of each security can always be bought or sold short. In the context of the normal distribution example the effective domain is $\Omega = \{(\theta_1,\theta_2) \in \bR^2 \:|\: \theta_2 > 0 \}$ if we re-define the second statistic to be $\phi_2(x) = -x^2$; this means that the security has a negative payoff for each share, and consequently the agent must be compensated to acquire such shares. Evaluating~(\ref{cost}), the cost function takes the form $C(\theta) = \frac{\theta_1^2}{4\theta_2} - \frac{1}{2}\log(2\theta_2)$.

 In the current market-based approach, the agent's chosen portfolio $\theta$ can also form the basis of a distribution over outcomes. 

The scoring rule~(\ref{expscore}) decomposes neatly into payoff and cost functions. The first term $\la \theta(\mu), \phi(x) \ra$ defines the agent's outcome-contingent reward, while the second term~$C(\theta(\mu))$ is the cost of acquiring a portfolio $\theta(\mu)$. We see here the duality between the approach of directly reporting $\mu$, or acquiring shares $\theta$: an agent reporting beliefs $\mu$ under scoring rule~(\ref{scoring}) would choose to acquire shares $\theta(\mu)$ in the information market with cost function~(\ref{cost}). Since~(\ref{scoring}) is a proper scoring rule, the information market with cost function~(\ref{cost}) is based on a proper market scoring rule.

%
\begin{proposition}
Let $C$ be the cumulant (i.e, cost function) for the exponential family corresponding to $\phi$, and let $\theta(\mu)$ be the optimal Lagrange multiplier for the mean constraints given an agent report of $\mu \in \mM$. Then the following scoring rule is equivalent to~(\ref{scoring}):
%
\begin{equation} \label{expscore}
S(\mu, x) = \la \theta(\mu), \phi(x) \ra - C(\theta(\mu)).
\end{equation}
%
\end{proposition}
%

\vfill
\pagebreak







\subsection{An Information-Theoretic Interpretation}\label{sec:infth}
 We observe a useful alternative view of the market 
scoring rule prediction market for exponential family learning. We connect the cost, payoff and profit function to information-theoretic 
quantities associated with the exponential family.

The following result has been previously pointed out by Amari \cite{Amari-KL}. 
\begin{theorem}\label{lem:profit_decomposition}{\bf (Profit Decomposition)}: Consider an exponential 
family $\family$ of distributions over some set of statistics $\suff(x)$, with natural parameters $\beta$. Let $\pi, \rho \in \family$ be any two probability distributions in the family. We use 
$\beta_\pi$ to denote the natural parameters of $\pi$ and $\mu_{\pi}$ to denote the expected value of the sufficient statistics under $\pi$. Likewise,
we can define $\beta_\rho$ and  $\mu_\rho$. Let $\lpf(\beta_{\rho})$ indicate the log partition function of $\rho$.
Let $H(\pi)$ denote the entropy of the distribution $\pi$, and 
 $K(\pi||\rho)$ denote the KL-divergence of $\rho$ relative to $\pi$.
Then, the following equality holds:  
\begin{equation}
 K(\pi||\rho) + H(\pi) = \lpf(\beta_{\rho}) - \beta_\rho \cdot \mu_\pi
\label{eq:profit_decomposition}
\end{equation}
\end{theorem}
\begin{proof}
\begin{eqnarray*}
 K(\pi||\rho) + H(\pi) &=& \int_x \pi(x)\log{\frac{\pi(x)}{\rho(x)}}dx\\
 &&\quad - \int_x \pi(x)\log{\pi(x)}dx \\
&=& - \int_x \pi(x)\log{\rho(x)}dx \\
&=& - \int_x \pi(x)\left[ \beta_\rho \cdot \suff(x) - \lpf(\beta_{\rho})\right]dx\\
&=& - \beta_\rho \cdot \int_x \pi(x) \suff(x) dx\\
&&\quad + \lpf(\beta_{\rho}) \int_x \pi(x) dx \\
&=&  \lpf(\beta_{\rho}) - \beta_\rho \cdot \mu_\pi
\end{eqnarray*}
\end{proof}

Equation~\ref{eq:profit_decomposition} gives us an alternative view of the
market scoring rule construction. Assume that $\pi$ is the true distribution, and consider two arbitrary
distributions $\rho_1, \rho_2 \in \family$. Note that $\lpf(\rho)$  is independent of $\pi$, and $\betavec_\rho \cdot \muvec_\pi$ is linear in the probabilities $\pi(x)$. If we want to measure loss by the KL-divergence, we can do so (in expectation) by setting a cost function that captures the first term, and defining security quantities ($\betavec$) and payoffs ($\suff$) to capture the second term.  In particular, in a market with cost function $\lpf$, if the
market price initially implies a distribution $\rho_1$, and a trader moves
the market to price than implies a distribution $\rho_2$, then the cost she
incurs is $\lpf(\rho_2) - \lpf(\rho_1)$. The number of securities bought to make this trade is given by the vector ($\betavec_{\rho_2} - \betavec_{\rho_1}$), and the expected payoff of the securities are given by $\muvec_\pi$. Thus, by equation \ref{eq:profit_decomposition}, the {\em net profit} of the trader is equal to $K(\pi||\rho_1) - K(\pi||\rho_2)$, {\it i.e.}, the reduction in KL-divergence with respect to the true distribution.  We note one useful property of this construction: For a fixed vector $\betavec$ of purchased securities, the cost is independent of the outcome 
(and outcome distribution $\pi$), while the payoff is independent of the initial market state in which these securities were purchased.
\subsection{Properties of the Maximum Entropy Market}
Consider the dual of the cost function, $C^{*}(\mu)$ defined as
$$C^{*}(\mu) = \sup_{\beta} \beta\cdot\mu - C(\beta)$$
This supremum is obtained at the value of $\beta$ for which $\mu = \nabla C(\beta)$; that is the natural parameter $\beta$ for which $\mu=\E_{p_{\beta}}[\phi(x)]$ is the mean parameter. Rewriting, 
$$C^{*}(\E_{p_{\beta}}[\phi(x)]) = \beta\cdot\E_{p_{\beta}}[\phi(x)] - C(\beta)$$
Thus, the mean parameters are the dual variables to the natural parameters.


\begin{thm} The value of the dual of the cost function is the negative entropy of the exponential family distribution obtained from backward mapping the mean parameters. \end{thm}
\proof
To see this, we note that for $p(x)=\exp\{\beta\cdot\phi(x)-C(\beta)\}$
\begin{eqnarray*}
-H(p) &=& \int_{x}p(x)\log p(x)\\
&=& \int_{x}p(x)[\beta\cdot\phi(x)-C(\beta)]\\
&=& \beta\cdot\int_{x}p(x)[\phi(x)]-\int_{x}p(x)C(\beta)\\
&=& \beta\cdot\E_{p}[\phi(x)]-C(\beta)
\end{eqnarray*}
\unproof
\rmk This result shows a nice parallel to LMSR, since the dual of the cost function of the LMSR is the negative entropy.\unrmk
\rmk The LMSR is essentially a special case applied to a multinomial distribution. The LMSR is known to have bounded ($\log n$) market maker loss. Thus, while in general the exponential family LMSR does not guarantee bounded market maker loss, for some special cases it can. 
\unrmk

{\thm Negative differential entropy $-H(p)\defeq \int_{x}p(x) \log p(x) dx$ is unbounded from above for an exponential family distribution.}
\proof
Note that, for an exponential family distribution
\begin{eqnarray*}
H(p) &=& -\int_{x}p(x)\log p(x)dx\\
&=& -\int_{x}p(x)[\beta\cdot \phi(x)-\psi(\beta)] dx\\
&=& -\beta\cdot\E[ \phi(x)]+\psi(\beta)
\end{eqnarray*}
If the range of $\beta$ is unbounded, the negative differential entropy is unbounded as well.
\unproof

%{\cor The exponential family market maker has unbounded worst case loss.}
%\proof From \cite{ACV13} we know that the worst case market maker loss can be expressed in terms of the conjugate of the cost function as 
%$$\sup_{\mu\in\phi(\mathcal{X})}C^{*}(\mu) - \min_{\mu\in\conv(\phi(\mathcal{X}))}C^{*}(\mu)$$ 
%Unbounded differential entropy for an exponential family distribution thus implies unbounded market maker loss.\unproof
\paragraph{Loss of the market maker}
First we derive an expression for the conjugate dual $C^{*}$ in terms of the primal variables.

Recall the definition of the conjugate dual:
$$C^{*}(\mu) = \sup_{q}q\cdot\mu - C(q)$$
The supremum is achieved at $q'$ such that $\nabla C(q')=\mu$. So we may rewrite:
$$C^{*}(\nabla C(q')) = \sup_{q}q\cdot\nabla C(q') - C(q)$$
This supremum is achieved at $q$ such that $\nabla C(q')=\nabla C(q)$. One such value of $q$ is $q'$.
So we have 
$$C^{*}(\nabla C(q')) = q'\cdot\nabla C(q') - C(q')$$
Also, recall that $\nabla C^{*}(\nabla C(q))=q$ for the exponential family market. (I think there are some restrictions on $q$.)

Let $q_{0}$ be the initial and $q_{f}$ the final market state. Then if $\mu$ is the expected value of the outcome sufficient statistics (\ie  the mean parameter) under the true distribution, the loss of the exponential family market maker can be written as:
\begin{eqnarray*}
\phi(x)(q_{f}-q_{0})-C(q_{f})+C(q_{0})&=& \phi(x) (q_{f}-q_{0})-q_{f}\nabla C(q_{f})+ C^{*}(\nabla C(q_{f}))\\
&&+q_{0}\nabla C(q_{0})- C^{*}(\nabla C(q_{0}))\\
&=&q_{f}(\phi(x) -\nabla C(q_{f}))+ C^{*}(\nabla C(q_{f}))\\
&&-q_{0}(\phi(x) -\nabla C(q_{0}))- C^{*}(\nabla C(q_{0}))\\
&=&-C^{*}(\nabla C(q_{0}))+ C^{*}(\phi(x))-q_{0}(\phi(x) -\nabla C(q_{0}))\\
&&+C^{*}(\nabla C(q_{f}))- C^{*}(\phi(x))+q_{f}(\phi(x) -\nabla C(q_{f}))\\
&=&  C^{*}(\phi(x))-C^{*}(\nabla C(q_{0}))-\nabla C^{*}(\nabla C(q_{0}))(\phi(x) -\nabla C(q_{0}))\\
&&-\left[C^{*}(\phi(x))-C^{*}(\nabla C(q_{f}))-\nabla C^{*}(\nabla C(q_{f}))(\phi(x) -\nabla C(q_{f}))\right]\\
&=&D_{C^{*}}[\phi(x),\nabla C(q_{0}))]-D_{C^{*}}[\phi(x),\nabla C(q_{f}))]
\end{eqnarray*}

\subsection{Example: Gaussian Markets}
{\ex We will now derive an expression for the dual of the log partition function for the Gaussian distribution. In this case,  $\beta$ is a $2$-dimensional  vector. Let $\beta=\begin{pmatrix}\beta_{1}\\ \beta_{2}\end{pmatrix}$ and $\mu=\begin{pmatrix}\mu_{1}\\ \mu_{2}\end{pmatrix}$. So 
\begin{eqnarray*}
\pmb{\mu}&=&\nabla\psi(\pmb{\beta})\\
&=&\nabla\left(-\frac{\beta_{1}^{2}}{4\beta_{2}}-\frac{1}{2}\log(-2\beta_{2})\right)\\
&=&\begin{pmatrix}-\frac{\beta_{1}}{2\beta_{2}}\\ \frac{\beta_{1}^{2}}{4\beta_{2}^{2}}-\frac{1}{2\beta_{2}}\end{pmatrix}
\end{eqnarray*}
Thus, $\pmb{\beta}$ can be written in terms of $\pmb{\mu}$ as follows:
$\beta_{1}=\frac{\mu_{1}}{\mu_{2}-\mu_{1}^{2}}$ and $\beta_{2}=\frac{1}{\mu_{1}^{2}-\mu_{2}}$. This leads to the following closed form expression for the dual of the log partition function:
$$\psi^{*}(\mu)=-\frac{1}{2}-\frac{1}{2}\log(\mu_{2}-\mu_{1}^{2})$$}
{\ex
We will now derive the expression for the differential entropy for the normal distribution. Recall that for the normal distribution $$p(x) = \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{\frac{-(x-\mu)^{2}}{2\sigma^{2}}\right\}$$
\begin{eqnarray*}
H(p) &=& -\int_{x}p(x)\log p(x)dx\\
&=& -\int_{x}p(x)\left[-\log(2\pi)-\log\sigma+\frac{-(x-\mu)^{2}}{2\sigma^{2}}\right]dx\\
&=& \log(2\pi)+\log\sigma+\frac{1}{2\sigma^{2}}\int_{x}p(x)(x-\mu)^{2}]dx\\
&=& \log(2\pi)+\log\sigma+\frac{1}{2\sigma^{2}}\E_{p(x)}[(x-\mu)^{2}]dx\\
&=& \log(2\pi)+\log\sigma+\frac{1}{2\sigma^{2}}\sigma^{2}dx\\
&=& \log(2\pi)+\frac{1}{2}+\log\sigma 
\end{eqnarray*}
Recall that the value of the dual of the log partition function is the value of the negative entropy of the distribution for an exponential family distribution. Thus when the variance of the normal distribution $\sigma^{2} = 0$, the value of the dual of the cost function goes to $\infty$ and is thus unbounded.}

{\ex For the normal distribution, the natural parameters $\pmb{\beta}=(\beta_{1},\beta_{2})$ can be written in terms of $\mu$ and $\sigma$ as $$\beta_{1}=\frac{\mu}{\sigma^{2}},\quad \beta_{2}=\frac{-1}{2\sigma^{2}}$$ and the log partition function is  $$\psi(\pmb{\beta})=-\frac{\beta_{1}^{2}}{4\beta_{2}}-\frac{1}{2}\log(-2\beta_{2})$$
Alternately, the log partition function can be written in terms of its variance and mean as $\frac{\mu^{2}}{2\sigma^{2}}+\log\sigma$.
Also note that the mean parameters are $\mu$ and $\mu^{2}+\sigma^{2}$.}
Let's now work out the loss for the Gaussian market maker. First note that for a Gaussian market for mean parameters $\begin{pmatrix}\mu_{1}\\ \mu_{1}^{2}+\sigma_{1}^{2}\end{pmatrix}$ and $\begin{pmatrix}\mu_{2}\\ \mu_{2}^{2}+\sigma_{2}^{2}\end{pmatrix}$, we have 
\begin{eqnarray*}
D_{C^{*}}\left(\begin{pmatrix}\mu_{1}\\ \mu_{1}^{2}+\sigma_{1}^{2}\end{pmatrix},\begin{pmatrix}\mu_{2}\\ \mu_{2}^{2}+\sigma_{2}^{2}\end{pmatrix}\right)
&=& C^{*}\left(\begin{pmatrix}\mu_{1}\\ \mu_{1}^{2}+\sigma_{1}^{2}\end{pmatrix}\right)-C^{*}\left(\begin{pmatrix}\mu_{1}\\ \mu_{1}^{2}+\sigma_{1}^{2}\end{pmatrix}\right)\\
&&-\nabla C^{*} \left(\begin{pmatrix}\mu_{2}\\ \mu_{2}^{2}+\sigma_{2}^{2}\end{pmatrix}\right)\begin{pmatrix}\mu_{1}-\mu_{2}\\ \mu_{1}^{2}+\sigma_{1}^{2}-\mu_{2}^{2}-\sigma_{2}^{2}\end{pmatrix}\\
&=& -\log \frac{\sigma_{1}}{\sigma_{2}}-\begin{pmatrix}\frac{\mu_{2}}{\sigma_{2}^{2}}\\ -\frac{1}{2\sigma_{2}^{2}}\end{pmatrix}\cdot \begin{pmatrix}\mu_{1}-\mu_{2}\\ \mu_{1}^{2}+\sigma_{1}^{2}-\mu_{2}^{2}-\sigma_{2}^{2}\end{pmatrix}\\
&=& -\log \frac{\sigma_{1}}{\sigma_{2}}-\frac{\mu_{2}(\mu_{1}-\mu_{2})-\frac{1}{2}(\mu_{1}^{2}-\mu_{2}^{2}+\sigma_{1}^{2}-\sigma_{2}^{2})}{\sigma_{2}^{2}}\\
%&=& \log \frac{\sigma_{2}}{\sigma_{1}}-\frac{\sigma_{1}^{2}-\sigma_{2}^{2}-(\mu_{1}-\mu_{2})^{2}}{2\sigma_{2}^{2}}\\
&=& \log \frac{\sigma_{2}}{\sigma_{1}}+\frac{\sigma_{2}^{2}-\sigma_{1}^{2}}{2\sigma_{2}^{2}}+\frac{(\mu_{1}-\mu_{2})^{2}}{2\sigma_{2}^{2}}
\end{eqnarray*}

So the Gaussian market maker loss is 
%parallels to quadratic market maker loss? 
\begin{eqnarray*}
D_{C^{*}}[\phi(x),\nabla C(q_{0}))]-D_{C^{*}}[\phi(x),\nabla C(q_{f}))]&=& \log \frac{\sigma_{0}}{\sigma_{x}}+\frac{\sigma_{0}^{2}-\sigma_{x}^{2}}{2\sigma_{0}^{2}}+\frac{(\mu_{x}-\mu_{0})^{2}}{2\sigma_{0}^{2}}\\
&&-\left[\log \frac{\sigma_{f}}{\sigma_{x}}+\frac{\sigma_{f}^{2}-\sigma_{x}^{2}}{2\sigma_{f}^{2}}+\frac{(\mu_{x}-\mu_{f})^{2}}{2\sigma_{f}^{2}}\right]\\
&=& \log \frac{\sigma_{0}}{ \sigma_{f}}+\frac{(x-\mu_{0})^{2}}{2\sigma_{0}^{2}}-\frac{(x-\mu_{f})^{2}}{2\sigma_{f}^{2}}\\
\end{eqnarray*}¥
Here we have used the fact that $\mu_{x}=x$ and $\sigma_{x}=0$.
\rmk In statistics, the standard score (aka z-score), $(x-\mu)/\sigma$, is the (signed) number of standard deviations an observation is above the mean. \unrmk
\pagebreak










\section{The Exponential Family Market Mechanism with Budgets}
In the previous section, we saw that we can define a cost-function based prediction market so that the aggregated belief of the traders represents the maximum likelihood estimate of the natural parameters of the true exponential family distribution.

In this section, we consider the the prediction market setup with traders that may be either informative or malicious. The malicious traders may want to inject faulty information into the market. The informative traders on the other hand receive points drawn from the true distribution on which they base their beliefs.

We will show that if we are able to impose finite initial budgets on the traders and control the market prices based on these budgets, then  it possible to set up the market so that it is prohibitive for damaging traders to participate in the market. Further, the informative traders can be shown to have expected growth in budget so that they are eventually able to move the market prices without restriction. 

In this section, we assume that the traders have exponential family beliefs. The cost function has the same form as the log partition function $T$ of the exponential family and the payoff is determined by the sufficient statistics of the data $\phi(x)$.
\subsection{Budget-limited Aggregation}
Imposing budget limits on the traders will allow us to control the amount of influence any one trader can have on moving the market prices. We will also satisfy an additional requirement that no trader has negative budget at any point of participation in the market. This is achieved by restricting the movement of the market and hence influencing the cost incurred by the trader. Recall that the payoff in this market is non-negative and hence the only adverse influence on a trader's budget is the cost of movement of the market state. 

In this section, we assume that the budget of each trader is known to the market maker, and that the market maker can directly limit the allowed trades based on a trader's budget. 
Let $\alpha$ be the budget of a trader in the market. Suppose with infinite budget, the trader would have moved the market state from $\theta$ to $\htheta$, where $\htheta$ represents his true belief. Let $C$ be the cost function. Now suppose further that $\alpha<C(\htheta)-C(\theta)$; that is the trader's budget does not allow for purchasing enough shares to move the market state to his belief. In this case, we want to budget-limit the trader's influence on the market state. 

We define the budget-limited final market state as $\theta'$. 
Here, we consider a specific functional form of $\theta'$: $$\theta'=\lambda\htheta+(1-\lambda)\theta$$  where $$\lambda=\min\left(1, \frac{\alpha}{C(\htheta) - C(\theta)}\right)$$ 

First, we  show that this trade is feasible given the trader's budget:
\begin{theorem}
Let the current market state be given by $\theta$. Let the final market state $\theta'=\lambda\htheta+(1-\lambda)\theta$ where $\lambda=\min\left(1, \frac{\alpha}{C(\htheta) - C(\theta)}\right)$. The cost to the trader to move the market state from $\theta$ to $\theta'$ is at most his budget $\alpha$.
\end{theorem}
\begin{proof}
From the convexity of $C$, we have
 $$C(\theta')\leq(1-\lambda)C(\theta)+\lambda C(\htheta)$$

Now 
\begin{eqnarray*}
C(\theta') - C(\theta) &\leq& (1-\lambda)C(\theta)\\
&&+\lambda C(\htheta) - C(\theta)\\
&=& \lambda\ (C(\htheta) - C(\theta))
\end{eqnarray*}

 Thus, $C(\theta') - C(\theta) \leq \alpha$. 
 \end{proof}
 
 We note that moving to $\theta'$ as defined may not be the optimal trade for a rational trader maximizing her expected profit. In general, the inequality above is strict, and so a trader does not fully exhaust her budget by moving to $\theta$. Our results below will continue to hold in the case that strategic informative traders move to a position closer to their beliefs $\htheta$.
\subsection{Damage Bound}
In this section, we will quantify the error in prediction that the market maker might have to endure as a result of malicious entities entering the market. We assume that these malicious entities trade in multiple instances of the market; thus the exposure of the market maker is over several \emph{rounds}. We use the standard log loss to measure this error in terms of the initial budget of traders.

We define the loss function for $\theta$ shares held:
$$L(\theta,x)=-\log(P_{\theta}(x))=\log\int \exp\{\theta^{T}\phi(x)\}\ dx-\theta^{T}\phi(x)=C(\theta)-\theta^{T}\phi(x)$$
%with the correspondence between $\beta$ and $\theta$ as defined earlier. %We extend this loss function to give zero loss in the absence of input. 



Suppose the prediction market runs over multiple rounds $t$. Let $\theta_{0}^{t}$ be the initial number of shares of each security that are held. Let $\hat{\theta}_{k}^{t}$ be the final values corresponding to the market state after the traders have made their reports. Let us assume that at this point the outcome is revealed; that is, we receive the value of the random variable $x^{t}$. 



Over multiple instances of the prediction market, we can track the change in budget of each trader. Let the budget at rounds $t$ and $t-1$ be $\alpha^{t}$ and $\alpha^{t-1}$ respectively. The change in budget for the trader is
\begin{eqnarray*}
\alpha^{t}-\alpha^{t-1}&=&C(\theta^{t})-C(\theta'^{t})-(\theta^{t}-\theta'^{t})^{T}\phi(x^{t})\\
&=&L(\theta,x^{t})-L(\theta',x^{t})
\end{eqnarray*}

Define the myopic impact of a trader $i$ in segment $t$ as
$$\Delta_{i}^{t}:=L(\hat{\theta}_{i-1}^{t},x^{t})-L(\hat{\theta}_{i}^{t},x^{t})$$
Thus, the myopic impact captures incremental gain in prediction due to the trader in a round. Note that the myopic impact caused by trader $i$ at round $t$ is equal to the change in his budget in that round.

The total myopic impact due to all $k$ active traders is given by
$$\Delta^{t}=L(\theta_{0}^{t},x^{t})-L(\hat{\theta}_{k}^{t},x^{t})$$
Thus $-\Delta^{t}$ captures the incremental loss of the market prediction after aggregation of all $k$ traders. 

\begin{theorem}
A coalition of $b$ malicious traders can at most cause loss bounded by their initial budgets.
\end{theorem}
\begin{proof}
Consider the myopic impact of a single trader $i$ after participating in the market $T$ times. Since the market evolves so that the budget of any trader never falls below zero, the total myopic impact in $T$ rounds caused due to trader $i$ is:
$$\Delta_{i}:=\sum_{t=1}^{T}\Delta_{i}^{t}= \sum_{t=1}^{T} (\alpha_{i}^{t} - \alpha_{i}^{t-1}) = \alpha_{i}^{T} - \alpha_{i}^{0}\geq -\alpha_{i}^{0}$$

Thus, any coalition of $b$ adversaries $\{1,\ldots,b\}$ can cause at most $\sum_{i=1}^{b}\alpha_{i}^{0}$ damage.
\end{proof}

This means that if it can be made prohibitively expensive for an attacker to generate clones, we can set up the prediction market with mostly informative traders. %imp

In Section \ref{sec:inf} we show that for an informative trader in every round, his budget increases in expectation. 
%We provide an information-theoretic justification for this claim in Section \ref{sec:infth}. 
The intuition behind this  is that a trader's prediction moves the input moves the market probability closer to the true probability distribution resulting in net expected profit.




\subsection{Budget of Informative Traders}\label{sec:inf}
Given the  information-theoretic interpretation of the cost-function based prediction market, we now show that the informative trader in the prediction market defined above increases his budget in a round in expectation {\em under his own belief distribution}.

We now characterize the expected change in budget for an informative trader. The following result holds for any round $t$; for simplicity, we have therefore dropped the superscript from the notation.
\begin{theorem}\label{thm:growth}
Let $\theta$ be the current market state in the exponential family prediction market. Suppose that an  informative trader with belief distribution  parametrized by $\theta'$  moves the market state to the budget-limited state $\htheta=\lambda \theta' + (1-\lambda) \theta$. Then, the expectation (over the trader's belief) of the trader's profit  is greater than zero whenever his budget is positive and his belief differs from the previous market position $\theta$. 
% Suppose that each informative trader gets a random sample of data, 
%resulting in a sequence of trader beliefs $\theta_i$, and hence a sequence of budget-limited market positions
%$\hat{\theta}_i$, before a final outcome $x$. Then, the expectation, over trader $i$'s belief distribution $\theta_i$, of trader $i$'s realized profit is greater than zero whenever her budget is positive and her belief differs from the previous market position $\hat{\theta}_{i-1}$. 
\end {theorem}
\begin{proof}
Let the cost function $C$ be equal to the log partition function $T$ of the belief distribution. The payoff is given by the sufficient statistics $\phi(x)$. Then, the trader's expected net payoff is given by
\begin{eqnarray*}
&& \E_{x\sim P_{\htheta}}[C(\theta)-C(\theta')-(\theta-\theta')\phi(x)]\\
&=& T(\theta)-\theta\E_{x\sim P_{\htheta}}[\phi(x)]-(T(\theta')-\theta' \E_{x\sim P_{\htheta}}[\phi(x)])\\
&=& T(\theta)-\theta\nabla T(\htheta)-(T(\theta')-\theta' \nabla T(\htheta))\\
&=& T(\theta)-T(\htheta)-\nabla T(\htheta)(\theta-\htheta)-(T(\theta')-T(\htheta)- \nabla T(\htheta)(\theta'-\htheta))\\
&=& D_{T}(\theta,\htheta)-D_{T}(\theta',\htheta)\\
&\geq&\lambda D_{T}(\theta,\htheta)\geq0
\end{eqnarray*}
The second to last inequality holds since $D_{T}(\theta',\htheta)$ is convex in $\theta'$ and we have:
\begin{eqnarray*}
D_{T}(\theta',\htheta)&=&D_{T}\left(\lambda\htheta+(1-\lambda)\theta,\htheta\right)\\
&\leq&\lambda D_{T}( \htheta ,\htheta)+ (1-\lambda) D_{T}( \theta ,\htheta)\\
&=&(1-\lambda) D_{T}( \theta ,\htheta)
%\implies D_{\psi}(\theta',\beta)+aD_{\psi}(\theta',\beta)&\leq& aD_{\psi}( \theta ,\beta)\\
\end{eqnarray*}¥
Thus, a trader who moves the market state can expect his profit to be positive and at least $\lambda D_{T}(\theta,\htheta)$.
 \end{proof}


For continuous distributions with a density, the probability that a trader with private information will form exactly the same beliefs as the current market position is $0$, and thus, each trader will have positive expected profit on almost all sequences of observed samples and beliefs. 
This result suggests that, eventually, every informative trader will have the ability to influence the market state in accordance with his beliefs, without being budget limited.

Notice that Theorem~\ref{thm:growth} only required that the market state to which the trader moves, be representable as a convex combination of the current market state and his belief. This means that the result holds for exponential utility traders aiming to maximize their utility. In this case, the trader who moves the market state can expect his profit to be positive and at least $\frac{1}{a}D_{T}(\theta,\htheta)$ where $a$ is the exponential utility parameter.

We note one important aspect of Theorem~\ref{thm:growth}: The expectation is taken with respect to each trader's belief at the time of trade, rather than with respect to the true distribution. This is needed because we have made no assumptions about the optimality of the traders' belief updating procedure. If we assume that the traders' belief formation is optimal, then this growth result will extend to the true distribution as well.
\pagebreak












\pagebreak
\section{Bayesian Traders with Linear Utility}
%\subsection{Conjugate Priors}

%In the previous section it's assumed that the agent has a belief $\q$, and moves the market state from initial state $\q_{init}$ to a convex combination $\tilde{\q} = \lambda \q + (1-\lambda)\q_{init}$. This note tries to justify this based on Bayesian updating, rather than budget limitations. (But it doesn't quite succeed.)
In this section, we lay the foundation for interpreting trader behavior in terms of Bayesian updates. 

As before, we are interested in eliciting the sufficient statistics of the data. We assume that the outcome is drawn from an exponential family distribution; the prediction market is setup as before with the cost function corresponding to the log partition function and the payoff function corresponding to the sufficient statistics. Thus, the market state provides an estimate on the natural parameter of the distribution from which the outcome is drawn. Additionally, we assume that the market also makes public the total number of traders that have traded in the market.

The goal is to aggregate information from risk neutral agents who have a belief distribution over the natural parameters. This prior distribution is updated by the agents based on the current market state, They also each have to access to the empirical mean of sufficient statistics based on a fixed number  $m$ of data points. Assuming a conjugate prior, both the prior and posterior belief distributions on the natural parameters are also an exponential family distributions. 

Now we will define the setup more formally. 

Let the data distribution be given by $p(x;\beta)=\exp\{\beta\cdot\phi(x)-T(\beta)\}$ where $T()$ is the log partition function and $\phi()$ are the sufficient statistics. Then the conjugate prior parametrized by $b_{0}=(n\nu, n)$ is given by $p(\beta;b_{0})=\exp\{b_{0}\cdot(\beta,\psi(\beta))-\psi(b_{0})\}$where $\psi()$ is the corresponding log partition function. The posterior distribution on the natural parameters is given by $p(\beta;b)$ where $b=(n\nu+m\hmu, n+m)$ where $\hmu$ is the empirical mean of the sufficient statistics of the $m$ data points drawn from the data distribution; that is, $\hmu=\sum_{i=1}^{m}\phi(x_{i})$. It turns out that 
\begin{equation}\label{eq:prior}
\E_{\beta\sim b_{0}}\E_{x\sim \beta}[\phi(x)]=\nu
\end{equation}¥ Thus, it is helpful to think of the prior as being based on a `phantom' sample of size $n$ and mean $\nu$. Thus the posterior mean is a convex combination of the prior and posterior means, and their relative weights depend on the phantom and empirical sample sizes.% need a reference here

Suppose the current market state is $\theta$ and $i$ traders have traded in the market when trader $i+1$ with prior belief distribution $p(\beta;b^{i}_{0})$ enters the market. Here $b^{i}_{0} = (n\nu_{i}, n)$. This trader also has access to private information in the form of empirical sufficient statistics $\hmu_{i}$ from $m$ data points. Recall from Proposition \ref{prop:exp} that natural parameter $\theta$ corresponds to expected statistics $\nabla T(\theta)$. Thus, he updates his belief as $p(\beta;b_{i})$ where $b=(m\hmu+mi \nabla T(\theta) + n\nu_{i}, n+mi+m)$. 
 
Suppose the trader wishes to maximize his expected payoff. Then the number of shares $\delta_{i}$ that he purchases when the current market state is $\theta$ is given by $$\arg\max_{\delta_{i}}\E_{\beta\sim b_{i}}\E_{x\sim \beta}\left[ \delta_{i}\phi(x) - T(\delta_{i} + \theta) + T(\theta)\right]$$
But, from Equation \ref{eq:prior} we have $\E_{\beta\sim b}\E_{x\sim \beta}[\phi(x)]=\frac{n\nu+m\hmu+mi \nabla T(\theta)}{ n+m(i+1)}$. To obtain the maximum, we set the gradient of the above expression with respect to $\delta_{i}$ to $0$. Thus, we have for the optimal number of shares $\delta_{i}^{*}$ $$\nabla T(\delta_{i}^{*} + \theta)=\frac{n\nu+m\hmu+mi \nabla T(\theta)}{ n+m(i+1)}$$ Thus, from Proposition \ref{prop:exp} we have that for an exponential family prediction market, the final market state is given by
$$\nabla G\left(\frac{n\nu+m\hmu+mi \nabla T(\theta)}{ n+m(i+1)}\right)$$ where $G$ is the convex conjugate of $T$. Thus, the final market state is a convex combination of prior, posterior and market means.

%Let $p(x;\theta)$ denote a probability density drawn from an exponential family with sufficient statistic $\phi : \mX \rightarrow \bR^d$, where $\theta$ is the natural parameter:
%$$
%p(x;\theta) = \exp \left[ \la \theta, \phi(x) \ra \right],
%$$
%and
%$$
%g(\theta) = \log \int_{\mX} \exp\la\phi(x),\theta\ra d\!x.
%$$
%Recall that $\grad g(\theta) = \Exp[\phi(x)]$ and $\grad^2 g(\theta) = \Var[\phi(x)]$. The family of conjugate priors is also an exponential family and takes the form
%$$
%p(\theta; n,\nu) = \exp\left[ \la n\nu,\theta \ra - ng(\theta) - h(\nu,n) \right].
%$$
%Here the feature map is $\psi(\theta) = (\theta, -g(\theta))$, the natural parameter is $(n\nu, n)$ where $n \in \bR$ and $\nu \in \bR^d$. The normalizer $h(\nu, n)$ is convex in $(n\nu,n)$. It is helpful to think of the prior as being based on a `phantom' sample of size $n$ and mean $\nu$. The justification for this is that
%$$
%\Exp_{\theta} \left[ \Exp_x \left[ \phi(x) | \theta \right]\right] 
%= \Exp_{\theta} \left[\grad g(\theta)\right]
%= \nu.
%$$
%(The proof is straightforward but not obvious. I have a reference for this but it's impossible to read---it's a mathematical statistics paper.)

%Suppose we draw a sample $X = (x_1,\ldots,x_m)$ of size $m$, and denote the empirical mean by $\mu[X] = \sum_{i=1}^m \phi(x_i)$. The posterior distribution is then
%$$
%p(\theta|X) \propto p(X|\theta)p(\theta|n,\nu) \propto \exp\left[ \la\mu[X]+n\nu,\theta\ra - (m+n)g(\theta) \right],
%$$
%and so the posterior mean is 
%\begin{equation} \label{posterior-mean}
%\frac{m\mu[X] + n\nu}{m+n}.
%\end{equation}


%In the context of prediction markets, one could imagine an agent who uses the current market estimate as its prior, and draws a empirical sample of size $m$. Its belief then takes the form~(\ref{posterior-mean}), where $n$ depends on the importance the agent places on the market estimate (perhaps based on how long the market has been running). However, note that the \emph{mean parameter} becomes a convex combination of market state and empirical belief, and this does not translate to the \emph{natural parameter}, which is what we would have liked. The new natural parameter is
%$$
%\grad g^{-1} \left(\frac{m\mu[X] + n\nu}{m+n}\right) = \grad g^* \left(\frac{m\mu[X] + n\nu}{m+n}\right).
%$$
%where $g^*$ is the convex conjugate of $g$.


%\subsection{Exponential Utility}
%Now suppose the trader has exponential utility, given by $$U(w) = -\frac{1}{a} \exp(-aw)$$ where $a$ is the coefficient of risk aversion. Suppose the trader wishes to maximize his expected utility. Then the number of shares $\delta$ that he purchases when the current market state is $\theta$ is given by 
%\begin{eqnarray*}
%&&\arg\max_{\delta}\E_{\beta\sim b}\E_{x\sim \beta}U\left[ \delta\phi(x) - C(\delta + \theta) + C(\theta)\right]\\
%&=&\arg\max_{\delta}\E_{\beta\sim b}\E_{x\sim \beta}-\frac{1}{a}\exp\left[ -a\delta\phi(x) +a C(\delta + \theta) -a C(\theta)\right]
%\end{eqnarray*}¥
%
%\begin{eqnarray*}
%\E_{\beta\sim b}\E_{x\sim \beta}\exp\left[-a\delta\phi(x)\right]&=&\int_{\mathcal{B}}\int_{\mathcal{X}}\exp[-a\delta\phi(x)]\, \exp\{\beta\cdot\phi(x)-\psi(\beta)\}\, dx\\
%&&\qquad\exp\{(n\nu+m\hmu)\cdot\beta-(n+m)\psi(\beta))-\Psi((n\nu+m\hmu, n+m))\}\, d\beta\\
%&=&\int_{\mathcal{B}}\exp\{(n\nu+m\hmu)\cdot\beta-(n+m)\psi(\beta))-\Psi((n\nu+m\hmu, n+m))\}\\
%&&\qquad\exp\{\psi(\beta-a\delta)-\psi(\beta)\}\,\, d\beta\int_{\mathcal{X}}\exp[(\beta-a\delta)\cdot\phi(x)-\psi(\beta-a\delta)]\, dx\\
%&=&\int_{\mathcal{B}}\exp\{(n\nu+m\hmu)\cdot\beta-(n+m)\psi(\beta))-\Psi((n\nu+m\hmu, n+m))\}\\
%&&\qquad\exp\{\psi(\beta-a\delta)-\psi(\beta)\}\,\, d\beta\\
%\end{eqnarray*}¥
\pagebreak

\section{Exponential Utility}

Assume the agent's belief distribution $p$ belongs to an exponential family, so it takes the form
%
\[
p(x;\theta) = \exp[\theta x - T(\theta)]
\]
%
where $\theta$ is the natural parameter and $T$ is the log-partition function. (I'm assuming that the sufficient statistic is $\phi(x) = x$ just for simplicity.) Assume also that the agent has an exponential utility for money $w$:
%
\[
U(w) = -\frac{1}{a} \exp(-aw).
\]
%
Here $a$ is the coefficient of risk aversion (higher means more risk averse, and the utility function is more concave).

%
\begin{proposition}\label{prop:exp-util}
An agent with exponential family belief with natural parameter $\htheta$, and exponential utility with coefficient $a$, makes a trade that moves the current market share vector $\theta$ to the convex combination $\frac{1}{1+a} \htheta + \frac{a}{1+a} \theta$ assuming an LMSR cost function.
\end{proposition}
%
%
\begin{proof}
Let $\delta$ be the vector of shares the agent trades. The payoff given eventual outcome $x$ is then $\delta x - C(\delta + \theta) + C(\theta)$. The utility for this payoff is as follows (recall that $\htheta$ is the agent's believed natural parameter).
%
\[ U( \delta x - C(\delta + \theta) + C(\theta) ) = -\frac{1}{a} \exp(-a \delta x + a C(\delta + \theta) - a C(\theta)).
\]
%
Taking the expected utility, we obtain
%
\begin{eqnarray*}
&   & \Exp\left[ U( \delta x - C(\delta + \theta) + C(\theta) ) \right] \\
& = & \int_{\mX} -\frac{1}{a} \exp(-a \delta x + a C(\delta + \theta) - a C(\theta)) \exp[\htheta x - T(\htheta)]\, dx \\
& = & -\frac{1}{a} \int_{\mX} \exp[(\htheta -a \delta) x + a C(\delta + \theta) - a C(\theta)) - T(\htheta)]\, dx \\
& = & -\frac{1}{a} \exp[a C(\delta + \theta) - a C(\theta)) + T(\htheta - a\delta) - T(\htheta)] \int_{\mX} \exp[(\htheta -a \delta) x - T(\htheta -a \delta)]\, dx \\
& = & -\frac{1}{a} \exp[a C(\delta + \theta) - a C(\theta)) + T(\htheta - a\delta) - T(\htheta)] \int_{\mX} p(x; \htheta -a \delta)\, dx \\
& = & -\frac{1}{a} \exp[a C(\delta + \theta) - a C(\theta)) + T(\htheta - a\delta) - T(\htheta)]\\
& = & U\left(- C(\delta + \theta) + C(\theta)) - \frac{1}{a} T(\htheta - a\delta) + \frac{1}{a} T(\htheta)\right)
\end{eqnarray*}
%
The second-last equality follows from the fact that $\int_{\mX} p(x; \htheta -a \delta)\, dx = 1$. Since utility $U$ is monotone increasing, it is maximized by maximizing its argument, which is a concave function of $\delta$ by convexity of $C$ and $T$. The optimality condition for the argument is
%
\begin{equation} \label{eq:optim}
\grad C(\delta^* + \theta) = \grad T(\htheta - a\delta^*)
\end{equation}
%
Now if the market maker is using LMSR, then $C$ is the log-partition function of the corresponding exponential family and $C = T$. Then~(\ref{eq:optim}) can be solved by equating the arguments. This leads to $\delta^* = (\htheta - \theta) / (1+a)$, which moves the share vector to $\theta + \delta^* = \frac{1}{1+a} \htheta + \frac{a}{1+a} \theta$.
\end{proof}
%

In the statement of the result I use the term ``LMSR cost function'' somewhat loosely, because we are not necessarily dealing with a market over exhaustive, mutually exclusive outcomes. What is meant is the cost function that arises by taking the dual to entropy of the maxent distribution with given mean parameter $\mu$. As we've discussed this seems like the right generalization of LMSR to arbitrary mean parameter spaces. 

Note that as $a \rightarrow 0$, we approach risk neutrality and the agent moves the share vector all the way to its private estimate $\htheta$. As $a$ grows (agent grows more risk averse) the agent makes smaller and smaller trades that keep it closer to the current estimate $\theta$.

So there is a kind of congruence between exponential family beliefs and exponential utility (as the names would suggest). 



\pagebreak








\section{Reinterpreting repeated trades}
In previous sections we have analyzed trader behavior as if it is his first entry into the market. In this section we will quantify how prior exposure in the market affects a trader's choices. In particular, we will show that a trader who has previously purchased shares in a market will, on subsequent entry, behave as if this purchase has updated his private belief. This result implies that any financial trade made in a market is equivalent to changing the trader's effective belief.

As before, we consider the exponential family prediction market where traders have exponential belief. Let us also suppose that traders in this market have exponential utility \[
U(w) = -\frac{1}{a} \exp(-aw).
\]
%
Here $a$ is the coefficient of risk aversion (higher means more risk averse, and the utility function is more concave).

Suppose an agent has exponential family belief parametrized by natural parameter $\hat{\theta}$. Based on this belief, let $\delta_{1}^{*}$ be the optimal vector of shares the agent decides to trade on first entering the market. Thus, his belief distribution is given by the density $$p(x;\htheta)=\exp(\la\hat{\theta}, \phi(x)\ra - T(\hat{\theta}))$$ where $T(\hat{\theta}) =\int_{\mathcal{X}}\exp\{\la\hat{\theta}, \phi(x)\ra\}\ dx$ is the log partition function and $\phi(x)$ are the sufficient statistics of the outcome $x\in \mathcal{X}$. On a subsequent entry into this market with market state $\theta'$, his optimal purchase $\delta_{2}^{*}$ is given by the solution of
%Here $\theta' = \theta+\delta_{1}^{*}+\delta'$ where $\delta'$ captures the movement of the share vector by other traders in the market.
\begin{eqnarray*}
&   & \arg\max_{\delta_{2}} \E_{x\sim p(x;\htheta)} U\left[ (\delta_{1}^{*}+\delta_{2} )\phi(x) - C(\delta_{1}^{*} + \theta) + C(\theta)- C(\delta_{2} + \theta') + C(\theta') \right] \\
& = & \arg\max_{\delta_{2}}\int_{\mathcal{X}} -\frac{1}{a} [\exp(-a (\delta_{1}^{*}+\delta_{2} )\phi(x) \\
&  & + a C(\delta_{1}^{*} + \theta) -a C(\theta)+a C(\delta_{2} + \theta') -a C(\theta')) \exp\{\hat{\theta} \phi(x) - T(\hat{\theta})\}]\, dx \\
& = & \arg\max_{\delta_{2}}\int_{\mathcal{X}} -\frac{1}{a} [\exp\{-a \delta_{2} \phi(x)+a C(\delta_{2} + \theta') -a C(\theta') \\
&  & + a C(\delta_{1}^{*} + \theta) -a C(\theta)\} \exp\{(\hat{\theta}-a \delta_{1}^{*}) \phi(x) - T(\hat{\theta})\}]\, dx \\
& = & \arg\max_{\delta_{2}}\int_{\mathcal{X}} U[N(\delta_{2},\theta')] \exp\{(\hat{\theta}-a \delta_{1}^{*}) \phi(x) - T(\hat{\theta})+ a C(\delta_{1}^{*} + \theta) -a C(\theta)\}]\, dx 
\end{eqnarray*}

Here, the first equality follows from the fact that we are taking expectation over the agent's belief parameter $\hat{\theta}$ and the second equality follows simply from rearranging the factors of $\phi(x)$. And lastly we have written $-\frac{1}{a} [\exp\{-a \delta_{2} \phi(x)+a C(\delta_{2} + \theta') -a C(\theta')]$ as $U[N(\delta_{2},\theta')]$ where $N(\delta_{2},\theta')$ is the net payoff when $\delta_{2}$ shares are purchased when the current market state is $\theta'$. 

Note that since $C=T$, $- T(\hat{\theta})+ a C(\delta_{1}^{*} + \theta) -a C(\theta)$ is proportional to $T(\hat{\theta}-a \delta_{1}^{*})$, it follows that $\int_{\mathcal{X}} \exp\{(\hat{\theta}-a \delta_{1}^{*}) \phi(x) - T(\hat{\theta})+ a C(\delta_{1}^{*} + \theta) -a C(\theta)\}]\, dx$ is proportional to the density with parameter $\hat{\theta}-a \delta_{1}^{*}$. Thus, picking the optimal share vector is equivalent to maximizing expected utility of $N(\delta_{2},\theta')$, where expectation is taken with respect to an exponential family distribution over $\mathcal{X}$ parametrized by $\hat{\theta}-a \delta_{1}^{*}$.




Let $\Theta\defeq\hat{\theta}-a \delta_{1}^{*}  $ be the effective belief. Thus we have that the trader chooses his share vector as follows.
\begin{eqnarray*}
&&  \arg\max_{\delta_{2}}\int_{\mathcal{X}} U[N(\delta_{2},\theta')] \exp\{(\hat{\theta}-a \delta_{1}^{*}) \phi(x) - T(\hat{\theta})+ a C(\delta_{1}^{*} + \theta) -a C(\theta)\}\, dx \\
& = &\arg\max_{\delta_{2}}\int_{\mathcal{X}} U[N(\delta_{2},\theta')] \exp\{(\hat{\theta}-a \delta_{1}^{*}) \phi(x) - T(\hat{\theta}-a \delta_{1}^{*})\}]\, dx\\
& = & \arg\max_{\delta_{2}} \int_{\mathcal{X}} -\frac{1}{a} \exp\{-a \delta_{2} \phi(x)+a C(\delta_{2} + \theta') -a C(\theta')\} \exp\{\Theta\cdot \phi(x) - T(\Theta)\}\, dx\\
& = & \arg\max_{\delta_{2}} U\left[- C(\delta_{2} + \theta') + C(\theta') - \frac{1}{a} T(\Theta - a\delta_{2}) + \frac{1}{a} T(\Theta)\right]\\
\end{eqnarray*}
This follows from Proposition \ref{prop:exp-util}. The maximizer is $\delta_{2}^* = (\Theta - \theta') / (1+a)$, which moves the share vector to $\theta' + \delta_{2}^* = \frac{1}{1+a} \Theta + \frac{a}{1+a} \theta'$ which is a convex combination of the effective belief and the current market state.

In other words, an exponential utility maximizing trader who has belief $\hat{\theta}$ with prior exposure $\delta$ in a market  will behave identically to an exponential utility maximizing trader with belief $\hat{\theta}-a \delta$ and no prior exposure in the market. Here $a$ is the utility parameter. This means that financial exposure can be equivalently understood as changing the privately held beliefs.
\pagebreak














\section{Equilibrium Market State for Exponential Utility Agents}
We have shown that every exponential-utility maximizing trader picks the share vector $\delta$ so that the eventual market state can be represented as a convex combination of the initial market state and the natural parameter of his (exponential family) belief distribution. In this section we will compute the equilibrium state in an exponential family market with multiple exponential-utility maximizing traders. 


Recall the following result from game theory. %need a reference/name for this result?
\begin{thm}\label{thm:potl}
Let $U_{i}(\vec{\delta})$ be the utility function of the $i^{th}$ trader given strategies $\vec{\delta}\defeq \delta_{1},\ldots,\delta_{i},\ldots,\delta_{n}$. If there exists a potential function $g(\vec{\delta})$ such that $$U_{i}(\vec{\delta})-U_{i}(\vec{\delta}_{-i},\delta_{i}')=g(\vec{\delta})-g(\vec{\delta}_{-i},\delta_{i}')$$
then when $g(\vec{\delta})$ is maximized, $\vec{\delta}$ is an equilibrium.
\end{thm}¥
In the exponential family market, the cost function $C$ is identical to the log partition function $T$.  Let $\vec{\delta}$ be the vector of vectors of shares purchased by every trader in the market when the market has reached equilibrium, $\theta$ the initial market state, $\htheta_{i}$ the natural parameter of trader $i$'s belief distribution and $a_{i}$ his utility parameter. 

Define a potential function as  $$g(\vec{\delta})\defeq  T(\theta+\sum_{i}\delta_{i})+\sum_{i}\frac{1}{a_{i}}T(\htheta_{i}-a_{i}\delta_{i})$$

%is the following expression for utility correct?
Now the utility of trader $i$ is $U_{i}(\vec{\delta})=- T(\theta+\sum_{j}\delta_{j}) + T(\theta+\sum_{j\neq i}\delta_{j}) - \frac{1}{a_{i}} T(\htheta_{i} - a_{i}\delta_{i}) + \frac{1}{a_{i}} T(\htheta_{i})$.
Thus, Theorem \ref{thm:potl} applies and we can find the equilibrium market state by maximizing $g(\vec{\delta})$ for each $\delta_{i}$.
\begin{eqnarray*}
\nabla_{\delta_{i}}g(\vec{\delta})&=&\nabla T(\theta+\sum_{j=1}^{n}\delta_{i})-\nabla T(\htheta_{i}-a_{i}\delta_{i})\\
&=&0
\end{eqnarray*}¥
This can be achieved by equating the arguments. That is, for each trader $i$, 
\begin{equation}\label{eq:eqbm}
\htheta_{i}-a_{i}\delta_{i}=\theta+\sum_{j=1}^{n}\delta_{j}
\end{equation}¥
Rewriting, we have for each trader $i$, 
$$\frac{\htheta_{i}}{a_{i}}-\delta_{i}=\frac{1}{a_{i}}(\theta+\sum_{j=1}^{n}\delta_{j})$$
Thus,
$$\sum_{i=1}^{n}\left(\frac{\htheta_{i}}{a_{i}}\right)-\sum_{i=1}^{n}\delta_{i}=\left(\theta+\sum_{j=1}^{n}\delta_{j}\right)\sum_{i=1}^{n}\frac{1}{a_{i}}$$
And
$$\sum_{j=1}^{n}\delta_{j}=
\frac{\sum_{i=1}^{n}\left(\frac{\htheta_{i}}{a_{i}}\right)-\theta\sum_{i=1}^{n}\left(\frac{1}{a_{i}}\right)}{1+\sum_{i=1}^{n}\frac{1}{a_{i}}}$$
Substituting in Equation \ref{eq:eqbm} we have the following expression for the final market state.
$$\theta+\sum_{j=1}^{n}\delta_{j}=
\frac{\theta+\sum_{i=1}^{n}\left(\frac{\htheta_{i}}{a_{i}}\right)}{1+\sum_{i=1}^{n}\frac{1}{a_{i}}}$$
Thus the equilibrium state is a convex combination of trader beliefs and initial market state.
\section{Conclusions}
%
%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\end{document}


