% Adjusted to add \usepackage[numbers]{natbib}
% \bibliographystyle{acmsmall} and \documentclass[prodmode,acmec]{ec-acmsmall}
% Jan 5, 2013 - David Parkes
%
% v2-acmsmall-sample.tex, dated March 6 2012
% This is a sample file for ACM small trim journals
%
% Compilation using 'acmsmall.cls' - version 1.3 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - March 2012

\documentclass[prodmode,acmec]{ec-acmsmall} % Aptara syntax

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Metadata Information
\acmVolume{9}
\acmNumber{4}
\acmArticle{39}
\acmYear{2013}
\acmMonth{6}
\usepackage[numbers]{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{amsthm}
\usepackage{parskip}
\usepackage{booktabs}

%new commands
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\mX}{\mathcal{X}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\grad}{\nabla}
\newcommand{\Exp}{\mathbf{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\q}{\mathbf{q}}
\newcommand{\mM}{\ensuremath{\mathcal{M}}}
\newcommand{\hmu}{\ensuremath{\hat{\mu}}}
\newcommand{\mP}{\ensuremath{\mathcal{P}}}
\newcommand{\mY}{\ensuremath{\mathcal{Y}}}
\newcommand{\mS}{\ensuremath{\mathcal{S}}}
\newcommand{\ds}{\displaystyle}
\newcommand{\mF}{\mathcal{F}}
\newcommand{\mD}{\mathcal{D}}
\newcommand{\htheta}{\hat{\theta}}
%---------------------------------------------------------------------------
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{con}[thm]{Conjecture}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{ex}[thm]{Example}
\newtheorem{qn}[thm]{Question}
\newcommand{\defn}{\bigbreak\noindent{\bf Definition. }}
\newcommand{\undefn}{\bigbreak}
\newcommand{\rmk}{\bigbreak\noindent{\bf Remark. }}
\newcommand{\unrmk}{\bigbreak}
%\newcommand{\proof}{\noindent{\bf Proof. }}
\newcommand{\unproof}{\hfill$\Box$\bigbreak}
\newcommand{\defeq}{\stackrel{\mathrm{def}}{=}}
\newcommand{\holds}[1]{\stackrel{?}{#1}}
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\ie} {{\it i.e. }}
\newcommand{\eg} {{\it e.g. }}
\newcommand{\etal} {{\it et al. }}
\newcommand{\E}{\mathbf{E}}
\newcommand{\betavec}{\pmb{\beta}}
\newcommand{\qvec}{\mathbf{q}}
\newcommand{\lpf}{\mathbf{\psi}} %logpartition function
\newcommand{\family}{\mathcal F} 
\newcommand{\suff}{\pmb{\phi}} % suff stats
\newcommand{\muvec}{\pmb{\mu}}
\newcommand{\st}{\hspace{10pt}\mathrm{s.t.}\hspace{10pt}}
\newcommand{\norm}[1]{|| #1 ||}
\newcommand{\tp}{\tilde{p}}


% Document starts

\begin{document}
% Page heads
\markboth{Abernethy et al.}{Maximum Entropy Prediction Markets}
% Title portion
\title{Maximum Entropy Prediction Markets}
\author{JACOB ABERNETHY
\affil{University of Michigan, Ann Arbor}
SINDHU KUTTY
\affil{University of Michigan, Ann Arbor}
S\'{E}BASTIEN LAHAIE
\affil{Microsoft Research, New York City}
RAHUL SAMI
\affil{University of Michigan, Ann Arbor}}
%%\author{GANG ZHOU
%%\affil{College of William and Mary}
%%YAFENG WU
%%\affil{University of Virginia}
%%TING YAN
%%\affil{Eaton Innovation Center}
%%TIAN HE
%%\affil{University of Minnesota}
%%CHENGDU HUANG
%%\affil{Google}
%%JOHN A. STANKOVIC
%%\affil{University of Virginia}
%%TAREK F. ABDELZAHER
%%\affil{University of Illinois at Urbana-Champaign}}
% NOTE! Affiliations placed here should be for the institution where the
%       BULK of the research was done. If the author has gone to a new
%       institution, before publication, the (above) affiliation should NOT be changed.
%       The authors 'current' address may be given in the "Author's addresses:" block (below).
%       So for example, Mr. Abdelzaher, the bulk of the research was done at UIUC, and he is
%       currently affiliated with NASA.

\begin{abstract}
In this paper, we draw connections between the aggregation performed by learning algorithms and the information aggregation done in prediction markets. We show that, under reasonable conditions, the behavior of rational traders can be understood as the result of performing a learning algorithm on their private data. Similarly,  the market state can be interpreted as a distribution over the outcome space. In particular, we show that a proper scoring rule can be derived from maximum entropy distributions. This scoring rule can be used as a general form of LMSR in prediction markets with over continuous outcome spaces. In order to provide insight on the behavior of rational traders in the market, we use the concept of exponential utility. We show that the traders' behavior can be understood as updating his belief using a Bayesian process and updating the market state in accordance with this utility function. These maxent prediction  markets can also be used to design  markets that are robust against adversarial traders. In fact, when traders are required to report their budgets and their beliefs, we can show that an informative trader eventually makes money and damaging traders eventually have limited influence in the market. Using ideas from convex analysis and the properties of the prediction market, we analyze the properties of the maxent market maker thus providing insight into the information content of the prediction market.
\end{abstract}

\category{J.4}{Social and Behavioral Sciences}{Economics}
\category{I.2.6} {Artificial Intelligence}{Learning}

\terms{Prediction Market Design, Machine Learning Algorithms}

\keywords{Exponential Families, Bayesian Learning, Maximum Entropy Distributions, generalized LMSR}

\acmformat{Jacob Abernethy, Sindhu Kutty, S\'{e}bastien Lahaie, and Rahul Sami, 2014. Maximum Entropy Prediction Markets}
%%Gang Zhou, Yafeng Wu, Ting Yan, Tian He, Chengdu Huang, John A. Stankovic, and Tarek F. Abdelzaher, 2010. A multifrequency MAC specially designed for  wireless sensor network applications.

% At a minimum you need to supply the author names, year and a title.
% IMPORTANT:
% Full first names whenever they are known, surname last, followed by a period.
% In the case of two authors, 'and' is placed between them.
% In the case of three or more authors, the serial comma is used, that is, all author names
% except the last one but including the penultimate author's name are followed by a comma,
% and then 'and' is placed before the final author's name.
% If only first and middle initials are known, then each initial
% is followed by a period and they are separated by a space.
% The remaining information (journal title, volume, article number, date, etc.) is 'auto-generated'.

\begin{bottomstuff}
%%This work is supported by the National Science Foundation, under
%%grant CNS-0435060, grant CCR-0325197 and grant EN-CS-0329609.
%%
%%Author's addresses: G. Zhou, Computer Science Department,
%%College of William and Mary; Y. Wu  {and} J. A. Stankovic,
%%Computer Science Department, University of Virginia; T. Yan,
%%Eaton Innovation Center; T. He, Computer Science Department,
%%University of Minnesota; C. Huang, Google; T. F. Abdelzaher,
%%(Current address) NASA Ames Research Center, Moffett Field, California 94035.
\end{bottomstuff}

\maketitle


\section{Introduction}
%------------------------------------------------------------
%Outline
%------------------------------------------------------------
%- exp family review
%- maxent scoring rule
%	- generalized lmsr
%	- does modularity (a la Hanson) still hold?
%- prediction market setup
%	- MLE with risk neutrality
%- conjugate prior market
%	- also on hyperparam?
%- exp-util
%- equilibrium for repeated trades
%- budgets and adversarial markets


We have dual goals in this paper. On the one hand, we highlight a structural similarity between prediction markets and exponential families. We see this as the syntax of our prediction market mechanism. On the other, this formulation has rich semantics as well: it allows for analysis of market behavior under various environments. For instance, we analyze market behavior with budget limited traders, traders that are exponential-utility-maximizers.
\section{Notation and Definitions}
convex conjugates, bregman divergences, scoring rules, sufficient statitics, exp families, Bayesian updates, conjugate prior.




\pagebreak

\section{Generalized Log Scoring Rules}


We consider a measurable space that consists of a set of outcomes $\mX$ together with a $\sigma$-algebra $\mF$. An agent or expert has a \emph{belief} over potential outcomes taking the form of a probability measure absolutely continuous with respect to a base measure~$\nu$.\footnote{Recall that a measure $P$ is absolutely continuous with respect to~$\nu$ if $P(A) = 0$ for every $A \in \mF$ for which $\nu(A) = 0$. In essence the base measure~$\nu$ restricts the support of $P$. In our examples $\nu$ will typically be a restriction of the Lebesgue measure for continuous outcomes or the counting measure for discrete outcomes.} Throughout we represent the belief as the corresponding density $p$ with respect to~$\nu$. Let $\mP$ denote the set of all such probability densities, and let $\mD \subseteq \mP$ be a subset from which the beliefs are drawn.

We are interested in eliciting information about the agent's belief, in particular expectation information. Let $\phi: \mX \rightarrow \bR^d$ be a vector-valued random variable or \emph{statistic}, where $d$ is finite. The aim is to elicit $\mu = \E_{p}[\phi(x)]$ where $x$ is the random outcome. A \emph{scoring rule} is a device for this purpose. Let $$\mM = \left\{ \mu \in \bR^d : \Exp_p[\phi(x)] = \mu,\, \mbox{for some $p \in \mP$} \right\}$$ be the set of realizable statistic expectations.  A scoring rule $S : \mM \times \mX \rightarrow \bR \cup \{-\infty\}$ pays the agent $S(\hmu,x)$ according to how well its report $\hmu \in \mM$ agrees with the eventual outcome $x \in \mX$. 
%
\begin{definition}
A scoring rule $S$ is  \emph{proper over domain $\mD$} for statistic $\phi$ if for each $\mu \in \mM$ and $p \in \mD$ with expected statistic $\mu$, we have
%
\begin{equation} \label{eq-proper}
\Exp_p[S(\mu,x)] \geq \Exp_p[S(\hmu,x)]
\end{equation}
%
for all alternative $\hmu$. If the domain is $\mD = \mP$, the set of all possible densities, then we simply say the scoring rule is \emph{proper}.
\end{definition}
%

\noindent
Note that given a proper scoring rule $S$ any affine transformation $\tilde{S}(\mu,x) = aS(\mu,x) + b(x)$ of the rule, with $a > 0$ and $b$ an arbitrary real-valued function of the outcomes, again yields a proper scoring rule termed \emph{equivalent}. Throughout we will often apply such affine transformations to obtain the clearest version of the scoring rule. We will also focus on scoring rules where inequality~(\ref{eq-proper}) is strict to avoid trivial cases such as constant scoring rules.

Our definition is more general than classical scoring rules in two respects. Classically, scoring rules take in the entire density $p$ rather than just some statistic, and incentive compatibility must hold over all of $\mP$. When the outcome space is large or infinite, it is not feasible to directly communicate $p$, so our definition allows for summary information of the belief. If there is a bijection between the sets $\mD$ and $\mM$, then we say that $\mM$ parametrizes $\mD$ and write $p(\cdot\,;\mu)$ for the density mapping to $\mu$.

Note that this definition of a scoring rule places only mild information requirements on the part of the agent to ensure truthful reporting. Because condition~(\ref{eq-proper}) holds for all $p$ consistent with expectation $\mu$, it is enough for the agent to simply know the latter and not the complete density to be properly incentivized. However, the agent must also agree that the density is actually drawn from domain $\mD$; if the scoring rule is proper, it is enough that the agent agree with the support of the density as implicitly defined by base measure $\nu$. 

When the outcome space is finite we can recover classical scoring rules from the definition by taking $\mD = \mP$ and using the statistic $\phi : \mX \rightarrow \{0,1\}^{\mX}$ that maps an outcome $x$ to a unit vector with a 1 in the component corresponding to $x$. The expectation of $\phi$ is then exactly the probability mass function.

\subsection{Proper Scoring from Consistency}

Our starting point for designing proper scoring rules is the classic logarithmic scoring rule for eliciting probabilities in the case of finite outcomes. This rule is simply $S(p,x) = \log p(x)$ and there are several ways to understand why it is proper, with different implications for generalization. 

First observe that the scoring rule compensates the agent with the log likelihood it assigned to the outcome, so the agent is maximizing the expected log likelihood via its report. Now it is a fundamental result in statistics that, under a variety of different sufficient conditions, the maximum likelihood estimator is statistically consistent; see~\cite{} for an overview of such conditions. This means that in the limit (as the size of the i.i.d.\ empirical sample increases) the log likelihood is maximized by the true parameters. By this reasoning, the following result follows immediately from the definition of consistency and the law of large numbers.
%
\begin{theorem} \label{max-like}
Suppose that $\mM$ parametrizes $\mD$. The logarithmic scoring rule defined by
%
\begin{equation} \label{log-score}
S(\mu, x) = \log p(x;\mu),
\end{equation}
%
where $\mu \in \mM$, is proper over $\mD$ if and only if the maximum likelihood estimator for $\mu$ is consistent.
\end{theorem}
%
We note that although our focus is on parametrizations by expectations, the preceding theorem in fact holds for more generic parameter spaces. The range of scoring rules it provides is broader than those characterized by~\cite{}, because it allows for a restriction over the set of beliefs. On the other hand, its applicability is limited to those domains $\mD$ that can be parametrized by the relevant statistics. The following provide concrete examples of these points. 
%
\begin{example}
Suppose the outcomes are supported on $[1,+\infty)$ and follow a Pareto distribution with density $f(x;\alpha) = \alpha/x^{\alpha+1}$ parametrized by an index $\alpha > 0$. The mean $\mu$ is related to the index via the one-to-one mapping $\mu = \frac{\alpha}{\alpha-1}$, so the density can alternatively be parametrized by the mean. Theorem~\ref{max-like} gives the following proper scoring rule for the mean of a Pareto distribution with support on $[1,+\infty)$:
%
\begin{equation} \label{pareto-scoring}
S(\mu,x) = \log \frac{\mu}{\mu-1} - \left(\frac{\mu}{\mu-1} + 1\right) \log x.
\end{equation}
%
We stress that the rule can only elicit the mean assuming the agent knows its belief is a Pareto distribution over $[1,+\infty]$. It does not elicit the mean of other families of densities parametrized by the mean (e.g., the exponential distribution). 
\end{example}
%
%
\begin{example}
Suppose the outcomes are supported on $(-\infty,+\infty)$ and follow a Cauchy distribution with density $f(x;m) = 1/\pi[1+(x-m)^2]$, parametrized by the median $m$. Theorem~\ref{max-like} leads to the following proper scoring rule for the median:
%
\begin{equation} \label{cauchy-scoring}
S(m,x) = -\log[1+(x-m)^2].
\end{equation}
%
Note that the median cannot be obtained as the expectation of any statistic. This highlights the range of parameters than may be elicited by the log score in different circumstances.
\end{example}
%

\subsection{Proper Scoring from Maximum Entropy}

We now turn to scoring rules that are proper over the entire domain of densities $\mP$. Our construction is the same as in the previous section: we take the log likelihood of a density parametrized by the relevant statistics. If the family of densities is chosen appropriately, the resulting rule will in fact hold over all $\mP$. To achieve this we draw on a well-known duality between maximum likelihood and maximum entropy.

\subsubsection*{Exponential Families}

We let $p(x;\mu)$ be the maximum entropy distribution with expected statistic $\mu$. Specifically, it is the solution to the following mathematical program:\footnote{We assume throughout that the minimum is finite and achieved for all $\mu \in \mM$. Some care is needed to ensure this holds for specific statistics and outcome spaces. For example, taking outcomes to be the real numbers, there is no maximum entropy distribution with a given mean $\mu$ (one can take densities tending towards the uniform distribution over the reals), but there is always a solution if we constrain both the mean and variance.}
%
\begin{equation} \label{maxent-prog}
 \min_{p \in \mP} F(p) \st \Exp_{p}[\phi(x)] = \mu,
\end{equation}
%
where the objective function is the negative entropy of the distribution, namely
%
\[ F(p) = \int_{x \in \mX} p(x) \log p(x)\, d\nu(x).
\]
%
Note that the explicit set of constraints in~(\ref{maxent-prog}) are linear, and to stress this fact we find it helpful to re-write them as $A_{\phi} p = \mu$, where $A_{\phi}$ is the expectation operator of statistics $\phi$. We let $G : \mM \rightarrow \bR$ be the optimal value function of~(\ref{maxent-prog}), meaning $G(\mu)$ is the (negative) entropy of the maximum entropy distribution with expected statistics $\mu$. 

It is well-known that solutions to~(\ref{maxent-prog}) are \emph{exponential family} distributions, whose densities with respect to $\nu$ take the form
%
\begin{equation} \label{exp-fam}
p(x;\theta) = \exp( \la \theta, \phi(x) \ra - T(\theta) ).
\end{equation}
%
The density is stated here in terms of its \emph{natural} parametrization $\theta \in \bR^d$, where $\theta$ arises as the Lagrange multiplier associated with linear constraints $A_{\phi}p = \mu$. The term $T(\theta)$ essentially arises as the multiplier for the normalization constraint (the density must integrate to 1), and so ensures that~(\ref{exp-fam}) is normalized:
%
\begin{equation} \label{log-part}
T(\theta) = \log \int_{\mX} \exp \la \theta, \phi(x) \ra \,d\nu(x).
\end{equation}
%
The function $T$ is known as the \emph{log-partition} or \emph{cumulant} function corresponding to the exponential family. Its domain is $\Theta = \{ \theta \in \bR^d : T(\theta) < +\infty \}$, called the natural parameter space. The exponential family is \emph{regular} if $\Theta$ is open---almost all exponential families of interest, and all those we consider in this work, are regular. The family is \emph{minimal} if there is no $\alpha \in \Theta$ such that $\la \alpha, \phi(x) \ra$ is a constant over $\mX$ ($\nu$-almost everywhere); minimality is a property of the associated statistic $\phi$, usually called the \emph{sufficient statistic} in the literature. 

The following proposition collects the relevant results on regular exponential families and their associated parameter domains and functions. A convex function $T$ is of \emph{Legendre type} if it is proper, closed, strictly convex and differentiable on the interior of its domain, and $\lim_{\theta \rightarrow \bar{\theta}} \norm{\grad T(\theta)} = +\infty$ when $\bar{\theta}$ lies on the boundary of the domain. 
%
\begin{prop}
Consider a regular exponential family with minimal sufficient statistic. The following properties hold:
\begin{enumerate}
\item $T$ and $G$ are of Legendre type, and $T = G^*$ (equivalently $G = T^*$). 
\item The gradient map $\grad T$ is one-to-one and onto the interior of $\mM$. Its inverse is $\grad G$ which is one-to-one and onto the interior of $\Theta$.
\item The exponential family distribution with natural parameter $\theta \in \Theta$ has expected statistic $\mu = \Exp_p[\phi(x)] = \grad T(\theta)$.
\item The maximum entropy distribution with expected statistic $\mu$ is the exponential family distribution with natural parameter $\theta = \grad G(\mu)$. 
\end{enumerate}
\end{prop}
%
In the above $T^*$ denotes as usual the convex conjugate of $T$, which here can be evaluated as $T^*(\mu) = \sup_{\theta \in \Theta} \la \theta, \mu \ra - T(\theta)$. Similarly, $G^*(\theta) = \sup_{\mu \in \mM} \la \theta, \mu \ra - G(\mu)$.

\subsubsection*{Proper Log Scoring}

We are now in a position to analyze the log scoring rule under exponential family distributions. From our discussion so far, we have that an exponential family density can be parametrized either by the natural parameter $\theta$, or by the mean parameter $\mu$, and that the two are related by the invertible gradient map $\mu = \grad T(\theta)$. We will write $p(x;\theta)$ or $p(x;\mu)$ given the parametrization used, which should be clear from context.

The following observation is crucial. Let $\tp \in \mP$ be a density (not necessarily an exponential family) with expected statistic $\mu$, let $p(\cdot\,;\mu)$ be the exponential family with the same expected statistic, and let $\mu' \in \mM$ be an alternative report. Then note that
%
\begin{equation} \label{equalizer-rule}
\Exp_{\tp}[\log p(x;\mu')] = \Exp_{p(\cdot;\mu)}[\log p(x;\mu')] = \la \theta', \mu \ra - T(\theta'),
\end{equation}
%
where $\theta' = \grad G(\mu')$ is the natural parameter for the exponential family with statistic $\mu'$. We see from this that the expected log score only depends on the expectation $\mu$ of the underlying density, not the full density, so when reasoning about the agent's reporting incentives we can just as well assume its beliefs are from an exponential family for simplicity.
%
\begin{theorem} \label{maxent-score}
Consider the logarithmic scoring rule $S(\mu,x) = \log p(x;\mu)$ defined over a set of densities $\mD$ parametrized by $\mM$. The scoring rule is proper over the entire domain of densities $\mP$ if and only if $\mD$ is the exponential family with statistic $\phi$. 
\end{theorem}
%
%
\begin{proof}
Let $\mu,\mu' \in \mM$ be the agent's true belief and an alternative report, and let $p,p' \in \mP$ be densities consistent with each belief. Let $\theta = \grad G(\mu)$ and $\theta' = \grad G(\mu')$, and note that $\mu = \grad T(\theta)$. We have
%
\begin{eqnarray*}
& & \Exp_p[\log p(x;\mu)] - \Exp_p[\log p(x;\mu')] \\
& = & \la \theta, \mu \ra - T(\theta) - \la \theta', \mu \ra + T(\theta') \\
& = & T(\theta') - T(\theta) - \la \theta' - \theta, \mu \ra \\
& = & T(\theta') - T(\theta) - \la \theta' - \theta, \grad T(\theta) \ra.
\end{eqnarray*}
%
The latter is positive by the strict convexity of $T$, which shows that the log score is proper. Now assume the defined log score is proper. By the Savage characterization of proper scoring rules for expectations (see~\cite{}), we must have
%
$$
S(\mu,x) = G(\mu) - \la \grad G(\mu), \mu - \phi(x) \ra
$$
for some strictly convex function $G$. Let $T = G^*$, so that $\grad G = {\grad T}^{-1}$, and let $\theta = \grad G(\mu)$. Then the above takes the form
%
\begin{eqnarray*}
\log p(x;\mu) & = & G(\mu) - \la \grad G(\mu), \mu - \phi(x) \ra \\
& = & \la \theta, \mu \ra - T(\theta) - \la \theta, \mu - \phi(x) \ra \\
& = & \la \theta, \phi(x) \ra - T(\theta),
\end{eqnarray*}
%
which shows that $p(x;\mu)$ takes the form of an exponential family.
\end{proof}
%
This theorem leads to a straightforward procedure for constructing score rules for expectations: define the relevant statistic, and consider the maximum entropy (equivalently, exponential family) distribution consistent with the agent's reported mean $\mu$. The agent is compensated according to the log likelihood of the eventual outcome according to the latter. The interpretation is that the agent is only providing partial information about the underlying density, so the principal first infers a full density according to the principle of maximum entropy, and then scores the agent using the usual log score.

An advantage of this generalization of the log score is that, for many statistics of interest, it leads to simple \emph{closed-form} formulas for the scoring rule. The following examples illustrate the construction.
%
\begin{example}
As base measure we take the Lebesgue restricted to $[0,+\infty)$, and we consider the statistic $\phi(x) = x$ so that we are simply eliciting the mean. The maximum entropy distribution with a given mean $\mu$ is the exponential distribution, and taking its log density gives the scoring rule
%
\begin{equation} \label{exp-scoring}
S(\mu, x) = -\frac{x}{\mu} - \log \mu.
\end{equation}
%
We stress that although this rule is derived from the exponential distribution, Theorem~\ref{maxent-score} implies that it elicits the mean of any distribution supported on the non-negative reals (e.g., Pareto, lognormal), in contrast to~(\ref{pareto-scoring}). Indeed, it is easy to see that the expected score~(\ref{exp-scoring}) depends only on the mean of the agent's belief because it is linear in $x$. 
%
Typically the exponential distribution is parametrized by its rate $\lambda = 1/\mu$. In that case the rule takes the form $S(\mu, x) = -\lambda x + \log \lambda$, and we see that $\lambda$ corresponds to the natural parameter.
\end{example}
%
%
\begin{example}
As a base measure we take the Lebesgue over the real numbers $\bR$. We are interested in eliciting the mean $\mu$ and variance $\sigma^2$, so as a statistic we take $\phi(x) = (x, x^2)$ for which $\Exp_p[\phi(x)] = (\mu, \mu^2+\sigma^2)$. The maximum entropy distribution for a given mean and variance is the normal distribution, and taking its log density gives the scoring rule
%
\begin{equation}
S((\mu, \sigma^2), x) = -\frac{(x-\mu)^2}{\sigma^2} - \log \sigma^2.
\end{equation}
%
Again, we stress that this scoring rule elicits the mean and variance of any density over the real numbers, not just those of a normal distribution. The construction easily generalizes to a multi-dimensional outcome space by taking the log density of the multivariate normal:
%
\begin{equation}
S(({\boldsymbol \mu}, {\boldsymbol \Sigma}), \boldsymbol{x}) = -( \boldsymbol{x} - {\boldsymbol \mu})'{\boldsymbol \Sigma}^{-1}( \boldsymbol{x} - {\boldsymbol \mu}) - \log |{\boldsymbol \Sigma}|.
\end{equation}
%
Here the statistics being elicited are the mean vector ${\boldsymbol \mu}$ and the covariance matrix ${\boldsymbol \Sigma}$. These scoring rules have been studied by Dawid and Sebastiani~\cite{} as rules that only depend on the mean and variance of the reported density. They note that these rules are weakly proper (because they do not distinguish between densities with the same first and second moments), but do not make the point that knowledge of the full density is not necessary on the part of the agent.
\end{example}
%












\pagebreak
\section{Maximum Entropy Market Making}


The purpose of a prediction market is to elicit and aggregate the beliefs (i.e., subjective probabilities) of agents over a space of \emph{outcomes}. In a single-agent setting, a scoring rule is used to elicit the agent's beliefs. In a multi-agent setting, an information market is used to aggregate the agent's beliefs. Hanson~\cite{} introduced the idea of a market scoring rule, which inherits the appealing elicitation and aggregation properties of both so that they can perform well in both thin and thick markets. In this work we derive a wide-ranging generalization of the logarithmic market scoring rule. Using a maximum entropy approach, we explain how a market scoring rule can be developed for outcome spaces both discrete and continuous, and for generic properties of the underlying distribution (e.g., mean and variance), rather than just the probabilities of individual outcomes. 



\subsection{Information Market}

We next consider how to indirectly elicit $\mu$ by setting up a market of contingent claim securities. Under this approach, the elements of the index set $\mS$ are interpreted not as statistics but as securities. The payoff from one share of security $s$ when outcome $x$ occurs is given by the mapping $\phi_s$. Thus if the vector of shares held by the agent is $\theta \in \bR^d$, where entry $\theta_s$ corresponds the number of shares of security $s$, then the payoff to the agent when $x$ occurs is evaluated by taking the inner product $\la \theta, \phi(x) \ra$. As a concrete example, recall our multinomial distribution example from the previous section, where $\phi_x(x') = 1$ if $x' = x$ and 0 otherwise. This means that security $x \in \mS$ pays 1 dollar if outcome $x \in \mX$ occurs, and nothing otherwise. (Such securities are known as Arrow-Debreu securities.) In our normal distribution example, the statistic for the mean had the mapping $\phi_1(x) = x$. Therefore a share of the corresponding security has a payoff that is linear in the outcome. (Such securities amount to futures contracts.) 

To extract useful information from such a securities market, we set a pricing scheme and examine the number of shares the agent chooses to acquire. Let $\Omega$ be the set of all portfolios (i.e., vectors of shares) that the agent can feasibly hold; we will see how this domain is determined in an instant. Each security $s$ has an associated price function $c_s : \Omega \rightarrow \bR$, which gives the marginal price $c_s(\theta)$ of security $s$ when the agent holds portfolio $\theta$. (Note that the marginal price depends on the entire portfolio $\theta$, not just the number of shares $\theta_s$.) A risk-neutral agent will choose to acquire shares up to the point where, for each share, expected payoff equals marginal price. Formally, if the agent acquires portfolio $\theta$, then for each $s \in \mS$ we must have
%
\begin{equation} \label{market}
\Exp_p[\phi_s(x)] = c_s(\theta).
\end{equation}
%
In this way, by its choice of $\theta$, the agent reveals that its belief is $\mu = c(\theta)$. There are several important properties that the price function $c$ should have. To simplify the agent's portfolio acquisition process, it should not be the case that the total cost of acquiring $\theta$ depends on the order in which shares are bought. This means that there should exist a \emph{cost function} $C : \Omega \rightarrow \bR$ such that $c = \nabla C$. The gradient $\nabla C$ must be onto $\mM$ to ensure that~(\ref{market}) always has a solution---this is the most important but technical condition to realize. To ensure that the solution is unique, it would also be convenient if the cost function were strictly convex. 
%
\begin{proposition}
The following cost function is monotone, convex, and onto $\mM$: 
%
\begin{equation} \label{cost}
C(\theta) = \log \int_{x \in \mX} \exp \left[\la \theta, \phi(x) \ra\right] \nu(dx).
\end{equation}
%
\end{proposition}
%
\noindent
Observe that in the context of our earlier multinomial example, cost function~(\ref{cost}) is exactly the cost function for Hanson's logarithmic market scoring rule. Our approach here provides a generalization of this market scoring rule to markets with contingent claim securities with arbitrary payoffs, designed to elicit specific properties of distributions, beyond just the probabilities of different outcomes. 

Because an agent would never select a portfolio with infinite cost, the effective domain of $C$ is $\Omega = \{\theta \:|\: C(\theta) < +\infty \}$. In the context of the multinomial distribution example, $\Omega = \bR^d$, so that shares of each security can always be bought or sold short. In the context of the normal distribution example the effective domain is $\Omega = \{(\theta_1,\theta_2) \in \bR^2 \:|\: \theta_2 > 0 \}$ if we re-define the second statistic to be $\phi_2(x) = -x^2$; this means that the security has a negative payoff for each share, and consequently the agent must be compensated to acquire such shares. Evaluating~(\ref{cost}), the cost function takes the form $C(\theta) = \frac{\theta_1^2}{4\theta_2} - \frac{1}{2}\log(2\theta_2)$.

Now, in our previous elicitation approach that used a scoring rule, the process of computing the scoring rule also provided a complete distribution $p(x;\mu)$ over outcomes, which could be used to infer other properties beyond just the agent beliefs $\mu$. In the current market-based approach, the agent's chosen portfolio $\theta$ can also form the basis of a distribution over outcomes. Consider the following distribution, represented as a density with respect to a base measure $\nu$:
%
\begin{equation} \label{expfam}
p(x;\theta) = \exp\left[ \la \theta, \phi(x) \ra - C(\theta) \right].
\end{equation}
%
Meaning, the probability that a subset $X \subseteq \mX$ of the outcomes occurs is $\int_{x \in X} p(x;\theta)\nu(dx)$. Observe that by definition~(\ref{cost}), this probability density indeed integrates to 1 over $\mX$, and it is clear that the density is non-negative as required.  

In the statistical literature a distribution that takes the form~(\ref{expfam}) is known as an \emph{exponential family}. The mapping $\phi$ is known as the \emph{sufficient statistic}, $\theta$ is the \emph{natural parameter}, and $C$ is the log-partition or \emph{cumulant} function. %This connection between information markets and exponential families allows us to draw on a vast statistical literature to understand the market's properties, and a growing computer science literature to apply efficient algorithms to compute costs and perform inference. 


%%%
%%%
%%%

\subsection{Duality}

There is a well-known duality between the maximum entropy approach and exponential families, which translates into a duality between the scoring rule and information market just developed. The duality implies that the approach leads to a \emph{market scoring rule}, applicable to both thin and thick multi-agent settings.

It is known that a distribution is a maximum entropy distribution if and only if it is an exponential family~\cite{}. To see this, let $\theta(\mu) \in \bR^d$ be the Lagrange multiplier corresponding to constraints~(\ref{mean}) when solving the maximum entropy program given the agent report $\mu$. (Our choice of notation is deliberately suggestive.) Let $A(\mu)$ be the Lagrange multiplier corresponding to~(\ref{normalize}). From the first order necessary conditions for optimality, we find that
%
\[ p(x;\mu) = \exp \left[ \la \theta(\mu), \phi(x) \ra - A(\mu) - 1 \right], \]
%
so that the solution is indeed in exponential family form. Conversely, given a cumulant $C$ from an exponential family and a parameter $\theta$, we see that $p(x;\theta)$ defined according to~(\ref{expfam}) is the maximum entropy distribution under constraints $\Exp_p[\phi(x)] = \mu$ where $\mu = \nabla C(\theta)$. We obtain the following.
%
\begin{proposition}
Let $C$ be the cumulant (i.e, cost function) for the exponential family corresponding to $\phi$, and let $\theta(\mu)$ be the optimal Lagrange multiplier for the mean constraints given an agent report of $\mu \in \mM$. Then the following scoring rule is equivalent to~(\ref{scoring}):
%
\begin{equation} \label{expscore}
S(\mu, x) = \la \theta(\mu), \phi(x) \ra - C(\theta(\mu)).
\end{equation}
%
\end{proposition}
%
\noindent
The scoring rule~(\ref{expscore}) decomposes neatly into payoff and cost functions. The first term $\la \theta(\mu), \phi(x) \ra$ defines the agent's outcome-contingent reward, while the second term~$C(\theta(\mu))$ is the cost of acquiring a portfolio $\theta(\mu)$. We see here the duality between the approach of directly reporting $\mu$, or acquiring shares $\theta$: an agent reporting beliefs $\mu$ under scoring rule~(\ref{scoring}) would choose to acquire shares $\theta(\mu)$ in the information market with cost function~(\ref{cost}). Since~(\ref{scoring}) is a proper scoring rule, the information market with cost function~(\ref{cost}) is based on a proper market scoring rule.

\section{The Exponential Family Market Mechanism: A Maximum Likelihood Approach}
In this section, we will set up a prediction market that aggregates beliefs from traders where the outcome is a continuous random variable. In particular, we assume that the outcome is drawn from an exponential family distribution. Each of the traders has access to a series of points drawn from this distribution. In other words, the traders have access an empirical mean of the sufficient statistics of the exponential family. Every trader has infinite budget so that the current market price after a trader has traded in the prediction market, exactly reflects his beliefs.

%We will now provide a more formal description of the problem statement.
%\subsection{Problem Statement}
%\begin{description}
%\item[Setup: ] Learning proceeds in rounds $j=1,2,\ldots$ where at the end of each round the algorithm may be provided with $x_{j}$  drawn from a set $\mathcal{X}$ according to some distribution $\mathcal{D}_{j}$, where $\mathcal{D}_{j}$ belongs to an exponential family defined by $k$ sufficient statistics.  In each round, some sequence of $n$ traders provide their estimates of the empirical means of each of the $k$ sufficient statistics drawn from $\mathcal{D}_{j}$.
%\item[Define: ] A learning algorithm that learns this distribution by providing a maximum likelihood estimate of its unknown natural parameters based on the empirical means.  
%\item[Proposed solution: ] Simulate a prediction market defined by a cost function $C$, securities $s_{0},s_{1},s_{2},\ldots, s_k$ and their payoffs $\phi_i(x)$ for $i=0,1,2,\ldots,k$ for the LMSR. Define a correspondence between the state of the prediction market and the output of the learning algorithm.
%\end{description}

For $x_{i}\sim P_{\betavec}$, recall that the likelihood function for independently drawn data $x_1,\ldots,x_n$ is given by $\prod_{i=1}^{n} P_{\betavec}(x_i)$. The maximum likelihood estimate of the natural parameters $\betavec$ is the value of the natural parameters that maximizes the likelihood function. 

We will now set up a prediction market with log market scoring rule (LMSR) and infinite budget traders such that the market state represents the maximum likelihood estimate (MLE) of the natural parameters of an exponential family distribution. %Given  under some interpretation of what information the participating agents have, and how they bet.

For a given exponential family distribution, the prediction market is defined as follows:
\begin{description}
\item[Traders] The prediction market will simulate a trader $i$ corresponding to expert $i$. This trader processes all information samples available to her directly or inferred from previous trades, and  forms a belief distribution such that the believed means of the sufficient statistics matches the empirical means of the sufficient statistics from the information samples. The trader trades in the market to maximize her expected payoff under her believed distribution.
\item[Securities and their payoff]
For each $i=1,2,\ldots,k$, we define a security $s_{i}$ with payoff $\phi_{i}(x)$ where $x$ is the ultimate outcome and $\Phi()$ defines the vector of sufficient statistics of the exponential family distribution according to which $x$ is drawn. We define an additional security $s_{0}$ with payoff $\phi_{0}(x):=a-\sum_{i=1}^{k}\phi_{i}(x)$ where $a$ is an appropriately chosen constant dependent on the range of $\Phi$ so that the payoff of $s_{0}$ is non-negative. 
\end{description}

We also note that since we want non-negative payoffs, we restrict the exponential families under consideration so that the sufficient statistics are lower bounded. If we have a constant lower bound on the sufficient statistics, then without loss of generality we can add a constant to each sufficient statistic without changing the exponential family in any way, as the constants will be absorbed in the log-partition function.

\paragraph{Arbitrage-free Property}
We can easily show that the prediction market we have defined is arbitrage-free: there is no sequence of trades that guarantees the trader a profit under all conditions. First, note that the incremental cost and payoff function are additive over multiple trades, so the net profit of a sequence of trades depends only on the initial and final market position, and is independent of the actual path along which the trade takes place.

Now, consider any trade (or sequence of trades) that moves the market from an initial state of $\qvec$ to a final state of $\qvec + \qvec'$. The cost of this trade is $C(\qvec + \qvec') - C(\qvec)$. The cost function of our market is the log-partition function of an exponential family, and thus, $C(.)$ is a strictly convex function~\cite{WainJordan08}. Thus, we have the inequality:
\[
   C(\qvec + \qvec') - C(\qvec)  > \qvec' \nabla C|_\qvec = \qvec' \mu,
 \]
 where $\mu$ is the mean sufficient statistics vector for the distribution with parameters $\qvec$.
The payoff due to this trade, with outcome $x$, is $\qvec'.\phi(x)$. 
Now, we observe that under the distribution with parameters $\qvec$, the expected payoff is $\qvec' E[\Phi(x)] = \qvec' \mu$. Thus, under this distribution over outcomes $x$, the expected profit is strictly less than $0$. This is only possible if the
realized profit is less than $0$ for at least one outcome $x$. As this statement is true for every $\qvec$ and $\qvec'$, and the profits are path-independent, the market is arbitrage-free.


%Let $\qvec^*=(q^{*}_{0},q^{*}_{1},\ldots,q^{*}_{k})$ be the number of shares of each security held by the traders. First we note that under the assumption of perfectly rational, risk-neutral traders with infinite budget and  beliefs as indicated above, the gradient of the cost function at this point is
 %$$\left(\frac{\partial C(\qvec)}{\partial q_{i}}\right)_{\qvec=\qvec^{*}}=\mu_{i}$$
%where $\mu_{i}$ is believed to be the expected payoff of the $i^{th}$ security by the last trader who traded in this market. We show below that the instantaneous price of security $i$, $\left(\frac{\partial C(\qvec)}{\partial q_{i}}\right)_{\qvec=\qvec^{*}}$ at any $\qvec=\qvec^{*}$, can be written as $\left(\frac{\partial\log (\int e^{\sum_{i=1}^{n}(q_{i}-q_{0})\phi_{i}(x)}dx)}{\partial q_{i}}\right)_{\qvec=\qvec^{*}}$. This means that the instantaneous price of the $i^{th}$ security only depends on the difference between the number of held securities of $i$ and 0. This ensures that if there exists an instantaneous price that reflects the belief of a trader at any vector $\qvec^{*}$, there must exist a $\qvec'$  that also reflects his beliefs and with the desirable property that $\forall i=0,1,2,\ldots,k\ q_{i}'\geq 0$. This additional property ensures that no trader will be required to short sell securities to move the market state to  reflect his beliefs.

\paragraph{Market State and Natural Parameters}
We define the interpretation function 
\begin{eqnarray}\label{eqn:int}
I(\qvec)&=&(q_{1}-q_{0},q_{2}-q_{0},\ldots,q_{k}-q_{0})\nonumber\\
&=&(\beta_{1}, \beta_{2},\ldots, \beta_{k})=\betavec
\end{eqnarray}
This allows us to interpret the state of the market in terms of a prediction on the natural parameters of the distribution.

\begin{theorem}
The natural parameter vector corresponding to the interpretation of the market state given by Equation \ref{eqn:int} is the vector of their maximum likelihood estimates. Further, this interpretation is unique.
\end{theorem}
\begin{proof}
We observe that for $i=1,\ldots,n$,
\begin{eqnarray*}
\mu_{i}&=&\left(\frac{\partial C(\qvec)}{\partial q_{i}}\right)_{\qvec=\qvec^{*}}\\
&=&\left(\frac{\partial\log\int \exp(\qvec^{T}\Phi(x))dx}{\partial q_{i}}\right)_{\qvec=\qvec^{*}}\\
&=&\left(\frac{\partial\log\int e^{q_{0}\phi_{0}(x)+\sum_{i=1}^{n}q_{i}\phi_{i}(x)}dx}{\partial q_{i}}\right)_{\qvec=\qvec^{*}}\\
&=&\left(\frac{\partial\log\int e^{q_{0}(a-\sum_{i=1}^{n}\phi_{i}(x))+\sum_{i=1}^{n}q_{i}\phi_{i}(x)}dx}{\partial q_{i}}\right)_{\qvec=\qvec^{*}}\\
&=&\left(\frac{\partial\log\int e^{q_{0}a} e^{\sum_{i=1}^{n}(q_{i}-q_{0})\phi_{i}(x)}dx}{\partial q_{i}}\right)_{\qvec=\qvec^{*}}\\
&=&\left(\frac{\partial\log (e^{q_{0}a} \int e^{\sum_{i=1}^{n}(q_{i}-q_{0})\phi_{i}(x)}dx)}{\partial q_{i}}\right)_{\qvec=\qvec^{*}}\\
&=&\left(\frac{\partial (q_{0}a)}{\partial q_{i}} \frac{\partial\log (\int e^{\sum_{i=1}^{n}(q_{i}-q_{0})\phi_{i}(x)}dx)}{\partial q_{i}}\right)_{\qvec=\qvec^{*}}\\
&=&\left(\frac{\partial \lpf(\betavec)}{\partial q_{i}}\right)_{\qvec=\qvec^{*}}\\
&=&\left(\frac{\partial \lpf(\betavec)}{\partial \beta_{i}}\frac{\partial \beta_{i}}{\partial q_{i}}\right)_{\qvec=\qvec^{*}}\\
&=&\left(\frac{\partial \lpf(\betavec)}{\partial \beta_{i}}\right)_{\rm \betavec=\betavec^{*}}
\end{eqnarray*}
Thus, this choice of parameters satisfies $\frac{\partial \lpf(\betavec)}{\partial \beta_{i}}=\mu_{i}$. We will now argue that this vector is unique. 

If the exponential family is represented so that there is a unique parameter vector associated with each distribution, the representation is said to be minimal. The Bernoulli, Gaussian, and Poisson distributions all have minimal representations. Now, for an exponential family whose representation is minimal, the gradient is an injection \cite[Prop. 3.2]{WainJordan08}. Thus, there is a unique parameter vector $\betavec$ that satisfies $\frac{\partial \lpf(\betavec)}{\partial \beta_{i}}=\mu_{i}$, for each $i\in\{1,\ldots,n\}$. Thus, if the $\mu_{i}$'s correspond to the empirical means, since our choice of $\betavec$ satisfies the equality, it must also be the vector corresponding to the MLE.
\end{proof}

Thus, we have shown that this prediction market (along with its strategic traders) aggregates trader information in a way that the market state can be interpreted as a predictive distribution (the maximum likelihood distribution) over an infinite outcome space.

It is worth noting here that if we can bound the parameter space and the sufficient statistics of the distribution, the market maker defined here has bounded worst case loss: The probability density, and hence log loss, would then be bounded for all market states that could be reached and all outcomes. In fact, for many practical applications, a distribution with unbounded parameter space is an approximation for a true space which itself may be bounded. In this case, worst case loss would thus be bounded.

\subsection{The Exponential Family Market Mechanism in Adversarial Markets}
In the previous section, we saw that we can define a cost-function based prediction market so that the aggregated belief of the traders represents the maximum likelihood estimate of the natural parameters of the true exponential family distribution.

In this section, we consider the the prediction market setup with traders that may be either informative or malicious. The malicious traders may want to inject faulty information into the market. The informative traders on the other hand receive points drawn from the true distribution on which they base their beliefs.

We will show that if we are able to impose finite initial budgets on the traders and control the market prices based on these budgets, then  it possible to set up the market so that it is prohibitive for damaging traders to participate in the market. Further, the informative traders can be shown to have expected growth in budget so that they are eventually able to move the market prices without restriction. 

\subsubsection{Budget-limited Aggregation}
Imposing budget limits on the traders will allow us to control the amount of influence any one trader can have on moving the market prices. We will also satisfy an additional requirement that no trader has negative budget at any point of participation in the market. This is achieved by restricting the movement of the market and hence influencing the cost incurred by the trader. Recall that the payoff in this market is non-negative and hence the only adverse influence on a trader's budget is the cost of movement of the market state. 

%There is distribution under which expected profit is $\leq 0$. Cost function is convex. 

%For any $\qvec$ there is a
In this section, we assume that the budget of each trader is known to the market maker, and that the market maker can directly limit the allowed trades based on a trader's budget. 
Let $\alpha$ be the budget of a trader in the market. Suppose with infinite budget, the trader would have moved the market state from $\qvec_{init}$ to $\qvec$, where $\qvec$ represents his true beliefs. Suppose further that $\alpha<C(\qvec)-C(\qvec_{init})$. In this case, we want to budget-limit the trader's influence on the market state. 

We define the budget-limited final market state as $\tilde{\qvec}$. 
Here, we consider a specific functional form of $\tilde{\qvec}$: $$\tilde{\qvec}=\lambda\qvec+(1-\lambda)\qvec_{init}$$  where $$\lambda=\min\Big(1, \frac{\alpha}{C(\qvec) - C(\qvec_{init})}\Big)$$ We first show that this trade is feasible given the trader's budget:
\begin{theorem}
Let the current market state be given by the vector $\qvec_{init}$ and $\tilde{\qvec}=\lambda\qvec+(1-\lambda)\qvec_{init}$. For $\lambda=\min\Big(1, \frac{\alpha}{C(\qvec) - C(\qvec_{init})}\Big)$, the cost to the trader to move the market state from $\qvec_{init}$ to $\tilde{\qvec}$ is at most his budget $\alpha$.
\end{theorem}
\begin{proof}
From the convexity of $C$, we have
 $$C(\tilde{\qvec})\leq(1-\lambda)C(\qvec_{init})+\lambda C(\qvec)$$

Now 
\begin{eqnarray*}
C(\tilde{\qvec}) - C(\qvec_{init}) &\leq& (1-\lambda)C(\qvec_{init})\\&&+\lambda C(\qvec) - C(\qvec_{init})\\
&=& \lambda\ (C(\qvec) - C(\qvec_{init}))
\end{eqnarray*}

%So when $\alpha<C(\qvec)-C(\qvec_{init})$, we pick $\lambda$ so that 
%$$\lambda\leq \frac{\alpha}{(C(\qvec) - C(\qvec_{init}))}$$
%
%where we want $\lambda$ such that
 Thus, $C(\tilde{\qvec}) - C(\qvec_{init}) \leq \alpha$. 
 \end{proof}
 
 We note that moving to $\tilde{\qvec}$ as defined may not be the optimal trade for a rational trader maximizing her expected profit. In general, the inequality above is strict, and so a trader does not fully exhaust her budget by moving to $\tilde{\qvec}$. Our results below will continue to hold in the case that strategic informative traders move to a position closer to their beliefs $\qvec$.
\subsubsection{Damage Bound}
Given this restriction on the initial budget of traders, we can now show that the total loss that can be induced by malicious traders is bounded. We define the loss function for $\qvec$ shares held as:
$$L(\qvec,x)=-\log(p_{\betavec}(x))=\log\frac{\int e^{\qvec^{T}\Phi(x)}\ dx}{e^{\qvec^{T}\Phi(x)}}$$
with the correspondence between $\betavec$ and $\qvec$ as defined earlier. %We extend this loss function to give zero loss in the absence of input. 

%{\bf Setup:} The market evolves in rounds $t=1,\ldots,T$. The end of a time segment occurs either when each expert has made his report or when the actual value of the random variable is received as input. Each expert $i$ reports the means of the sufficient statistics as available to him at time $t$. These are combined with the previously reported means to yield $\mu_{0}^{t}(x),\mu_{1}^{t}(x),\ldots,\mu_{k}^{t}(x)$ and an associated weight $w_{i}^{t}$ which can be interpreted as the number of points from which these values have been determined. These sufficient statistics correspond to a particular value of $B$ and hence of $\qvec$, the number of shares of each security that are held by traders. Since trader $i$ has finite budget $\alpha_{i}^{t}$ at time $t$, the current market state is moved to $\qvec'$ instead, so that cost of moving it to $\qvec'$ is within his budget.

%For an initial positive budget of $\alpha_{i}^{0}$, we perform updates to the budgets of each agent as
%$$\alpha_{i}^{t}:=\alpha_{i}^{t-1}+\lambda_{i}^{t}(L(\tilde{q}_{i-1}^{t},x^{t})-L({q}_{i}^{t},x^{t}))$$ where we define
%$$\lambda_{i}^{t}:=\min\Big(1, \frac{\alpha_{i}^{t-1}}{C(\qvec_{i}) - C(\tilde{\qvec}_{i-1})}\Big)$$

Suppose the prediction market runs over multiple rounds $t$. Let $\qvec_{0}^{t}$ be the initial number of shares of each security that are held. Let $\tilde{\qvec}_{k}^{t}$ be the final values corresponding to the market state after the traders have made their reports. Let us assume that at this point we receive the value of the random variable $x^{t}$. 


%Let the loss function be $L(\cdot,\cdot)$, where the two arguments are the vector representing the number of shares held of each security and the actual value of the random variable. 

%We want to show:
%$$L({\qvec}_{0}^{t},x^{t})-L(\tilde{\qvec}_{k}^{t},x^{t})+\sum_{i}\alpha_{i}^{t-1}\leq \sum_{i}\alpha_{i}^{t}$$
%That is, the loss due to incorporating the budget-limited information of the traders, is not more than the increase in their budgets.
Over multiple instances of the prediction market, we can track the change in budget of each trader. The change in budget for trader $i$ is
\begin{eqnarray*}
\alpha_{i}^{t}-\alpha_{i}^{t-1}&=&C(\tilde{\qvec}_{i-1}^{t})-C(\tilde{\qvec}_{i}^{t})-(\tilde{\qvec}_{i-1}^{t}-\tilde{\qvec}_{i}^{t})^{T}\Phi(x^{t})\\
&=&L(\tilde{\qvec}_{i-1}^{t},x^{t})-L(\tilde{\qvec}_{i}^{t},x^{t})
\end{eqnarray*}

Define the myopic impact of a trader $i$ in segment $t$ as
$$\Delta_{i}^{t}:=L(\tilde{\qvec}_{i-1}^{t},x^{t})-L(\tilde{\qvec}_{i}^{t},x^{t})$$
Thus, the myopic impact captures incremental gain due to the trader in a round. Note that the myopic impact caused by trader $i$ at round $t$ is equal to the change in his budget in that round.

The total myopic impact due to all $k$ active traders is given by
$$\Delta^{t}=L(\qvec_{0}^{t},x^{t})-L(\tilde{\qvec}_{k}^{t},x^{t})$$
Thus $-\Delta^{t}$ captures the incremental loss due to the predictive probability after aggregation of all $k$ traders. 

%The myopic impact of a trader $i$ in segment $t$, is given by
%$$\Delta_{i}^{t}:=L(\tilde{B}_{i-1}^{t},x^{t})-L(\tilde{B}_{i}^{t},x^{t})$$
%where $\tilde{B}_{i}$ is the vector of sufficient statistics corresponding to the the budget-limited state of the market as induced by trader $i$.
%The total myopic impact due to all $k$ active traders is thus given by
%$$\Delta^{t}=L(B_{0}^{t},x^{t})-L(\tilde{B}_{k}^{t},x^{t})$$

%If the loss function is convex in the first argument, then we have:
%\begin{eqnarray*}
%\Delta_{i}^{t}&=&L(\tilde{B}_{i-1}^{t},x^{t})-L(\tilde{B}_{i}^{t},x^{t})\\
%&=&L(\tilde{q}_{i-1}^{t},x^{t})-L(\tilde{q}_{i}^{t},x^{t})\\
%&=&L(\tilde{q}_{i-1}^{t},x^{t})-L(\lambda_{i}{q}_{i}^{t}+(1-\lambda_{i})\tilde{q}_{i-1}^{t},x^{t})\\
%&\geq&L(\tilde{q}_{i-1}^{t},x^{t})-\lambda_{i}L({q}_{i}^{t},x^{t})-(1-\lambda_{i})L(\tilde{q}_{i-1}^{t},x^{t})\\
%&=&\lambda_{i}(L(\tilde{q}_{i-1}^{t},x^{t})-L({q}_{i}^{t},x^{t}))\\
%&=& \alpha_{i}^{t} - \alpha_{i}^{t-1}
%\end{eqnarray*}
%Thus, the myopic damage caused by trader $i$ at round $t$ is upper-bounded by the change in his budget in that round.


%If we pick a loss function that is also bounded in $[0,1]$, then we see that the budget never falls below zero. 
\begin{theorem}
A coalition of $b$ malicious traders can at most cause loss bounded by their initial budgets.
\end{theorem}
\begin{proof}
Consider the myopic impact of a single trader $i$ after participating in the market $T$ times. Since the market evolves so that the budget of any trader never falls below zero, the total myopic impact in $T$ rounds caused due to trader $i$ is:
$$\Delta_{i}:=\sum_{t=1}^{T}\Delta_{i}^{t}= \sum_{t=1}^{T} (\alpha_{i}^{t} - \alpha_{i}^{t-1}) = \alpha_{i}^{T} - \alpha_{i}^{0}\geq -\alpha_{i}^{0}$$

Thus, any coalition of $b$ adversaries $\{1,\ldots,b\}$ can cause at most $\sum_{i=1}^{b}\alpha_{i}^{0}$ damage.
\end{proof}

This means that if it can be made prohibitively expensive for an attacker to generate clones, we can set up the prediction market with mostly informative traders. 

In Section \ref{sec:inf} we show that for an informative trader in every round, his budget increases in expectation. We provide an information-theoretic justification for this claim in Section \ref{sec:infth}. The intuition behind this claim is that his prediction moves the input moves the market probability closer to the true probability distribution resulting in net expected profit.


\subsubsection{An Information-Theoretic Interpretation}\label{sec:infth}
 We observe a useful alternative view of the market 
scoring rule prediction market for exponential family learning. We connect the cost, payoff and profit function to information-theoretic 
quantities associated with the exponential family.

The following result has been previously pointed out by Amari \cite{Amari-KL}. 
\begin{theorem}\label{lem:profit_decomposition}{\bf (Profit Decomposition)}: Consider an exponential 
family $\family$ of distributions over some set of statistics $\suff(x)$, with natural parameters $\betavec$. Let $\pi, \rho \in \family$ be any two probability distributions in the family. We use 
$\betavec_\pi$ to denote the natural parameters of $\pi$ and $\muvec_{\pi}$ to denote the expected value of the sufficient statistics under $\pi$. Likewise,
we can define $\betavec_\rho$ and  $\muvec_\rho$. We abuse notation slightly and let $\lpf(\rho)$ indicate the log partition function of $\rho$ which technically depends on its natural parameters.
Let $H(\pi)$ denote the entropy of the distribution $\pi$, and 
 $K(\pi||\rho)$ denote the KL-divergence of $\rho$ relative to $\pi$.
Then, the following equality holds:  
\begin{equation}
 K(\pi||\rho) + H(\pi) = \lpf(\rho) - \betavec_\rho \cdot \muvec_\pi
\label{eq:profit_decomposition}
\end{equation}
\end{theorem}
\begin{proof}
\begin{eqnarray*}
 K(\pi||\rho) + H(\pi) &=& \int_x \pi(x)\log{\frac{\pi(x)}{\rho(x)}}dx\\
 &&\quad - \int_x \pi(x)\log{\pi(x)}dx \\
&=& - \int_x \pi(x)\log{\rho(x)}dx \\
&=& - \int_x \pi(x)\left[ \betavec_\rho \cdot \suff(x) - \lpf(\rho)\right]dx\\
&=& - \betavec_\rho \cdot \int_x \pi(x) \suff(x) dx\\
&&\quad + \lpf(\rho) \int_x \pi(x) dx \\
&=&  \lpf(\rho) - \betavec_\rho \cdot \muvec_\pi
\end{eqnarray*}
\end{proof}

Equation~\ref{eq:profit_decomposition} gives us an alternative view of the
market scoring rule construction. Assume that $\pi$ is the true distribution, and consider two arbitrary
distributions $\rho_1, \rho_2 \in \family$. Note that $\lpf(\rho)$  is independent of $\pi$, and $\betavec_\rho \cdot \muvec_\pi$ is linear in the probabilities $\pi(x)$. If we want to measure loss by the KL-divergence, we can do so (in expectation) by setting a cost function that captures the first term, and defining security quantities ($\betavec$) and payoffs ($\suff$) to capture the second term.  In particular, in a market with cost function $\lpf$, if the
market price initially implies a distribution $\rho_1$, and a trader moves
the market to price than implies a distribution $\rho_2$, then the cost she
incurs is $\lpf(\rho_2) - \lpf(\rho_1)$. The number of securities bought to make this trade is given by the vector ($\betavec_{\rho_2} - \betavec_{\rho_1}$), and the expected payoff of the securities are given by $\muvec_\pi$. Thus, by equation \ref{eq:profit_decomposition}, the {\em net profit} of the trader is equal to $K(\pi||\rho_1) - K(\pi||\rho_2)$, {\it i.e.}, the reduction in KL-divergence with respect to the true distribution.  We note one useful property of this construction: For a fixed vector $\betavec$ of purchased securities, the cost is independent of the outcome 
(and outcome distribution $\pi$), while the payoff is independent of the initial market state in which these securities were purchased.

%Thus, we have shown that there exist deep connections between learning the natural parameters of an exponential family distribution and cost function based prediction markets. In Chapter \ref{nonmyopic} we exploit this connection and propose a bounded regret learning algorithm in a Bayesian model under less-than-ideal conditions: where the experts may be adversarial and strategic. 

\subsubsection{Budget of Informative Traders}\label{sec:inf}
Given the  information-theoretic interpretation of the cost-function based prediction market, we now show that the informative trader in the prediction market defined above increases his budget in a round in expectation {\em under his own belief distribution}.

%For this, we need the following simple extension of Jensen's inequality that follows directly from the definition of convexity of a function.
%\begin{lemma}\label{lem:convex}
%For a strictly convex function $f$ with unique minimum $x_{min}$, $x\neq x_{min}$ in the domain of $f$, and $\lambda \in (0,1]$,
%$$f(\lambda x_{min}+ (1-\lambda)x)< f(x)$$
%\end{lemma}
%\begin{proof}
%\begin{eqnarray*}
%f(\lambda x_{min}+ (1-\lambda)x) &<& \lambda f(x_{min})+ (1-\lambda)f(x)\\
%&=& \lambda(f(x_{min})-f(x)) +f(x)\\
%&\leq& f(x)
%\end{eqnarray*}
%\end{proof}

We now characterize the expected change in budget for an informative trader. The following result holds for any round $t$; for simplicity, we have therefore dropped the superscript from the notation.
\begin{theorem}\label{thm:growth}
 Suppose that each informative trader gets a random sample of data, 
resulting in a sequence of trader beliefs $\qvec_i$, and hence a sequence of budget-limited market positions
$\tilde{\qvec}_i$, before a final outcome $x$. Then, the expectation, over trader $i$'s belief distribution $\qvec_i$, of trader $i$'s realized profit is greater than zero whenever her budget is positive and her belief differs from the previous market position $\tilde{\qvec}_{i-1}$. 
\end {theorem}
\begin{proof}
Trader $i$'s believed distribution is the distribution parametrized by $\qvec_i$.
Therefore, the expected profit, over possible outcome values $x$, for a trader $i$ in a given round is given by
\begin{eqnarray*}
&& E[C(\tilde{\qvec}_{i-1})-C(\tilde{\qvec}_{i})-(\tilde{\qvec}_{i-1}-\tilde{\qvec}_{i})\Phi(x)]\\
&=& K(\qvec_{i}||\tilde{\qvec}_{i-1})] + H(\qvec_{i}) - [K(\qvec_{i}||\tilde{\qvec}_{i}) + H(\pi)]]\\
&=& K(\qvec_{i}||\tilde{\qvec}_{i-1}) - K(\qvec_{i}||\tilde{\qvec}_{i})
\end{eqnarray*}
We recall that $\tilde{\qvec}_{i}$ can be expanded as:
$$\tilde{\qvec_{i}} = \lambda \qvec_{i} + (1- \lambda) \tilde{\qvec}_{i-1}$$ where $\lambda$ is strictly greater than $0$, but no more than $1$. It is a standard result from information theory that $K(.,.)$ is a convex function, and therefore, we have:
\[
K(\qvec_{i}||\tilde{\qvec}_{i-1}) \leq \lambda K(\qvec_{i}||\qvec_{i}) + (1-\lambda) K(\qvec_{i}||\tilde{\qvec}_{i-1})
\]
The first term on the right hand side is zero, and thus, we get:
\[
  K(\qvec_{i}||\tilde{\qvec}_{i-1}) - K(\qvec_{i}||\tilde{\qvec}_{i}) \geq \lambda K(\qvec_{i}||\tilde{\qvec}_{i-1})
\]
%Thus, it is sufficient to prove that E_{\qvec}[K(\pi||\tilde{\qvec}_{i-1}^{t}] >= E_{\qvec}[K(\pi||\tilde{\qvec}_{i}^{t}].
 \end{proof}
%\begin{theorem}
%The expected budget of an informative trader increases when he participates in the market. Here, expectation is taken with respect to the true distribution from which the outcome is drawn.
%We show that the budget of an informative trader grows in expectation:\end{theorem}
%\begin{proof}
%Let $\pi$ be the true distribution from which the outcome $x^{t}$ is drawn. Let $K(\pi||\qvec)$ be the relative entropy between the true distribution $\pi$ and the predictive distribution determined by the securities vector $\qvec$. Then the expected change in budget for a trader $i$ in round $t$ is given by
%\begin{eqnarray*}
%E_{x,\qvec}[\alpha_{i}^{t}&-&\alpha_{i}^{t-1}]\\
%&=&E_{x,\qvec}[C(\tilde{\qvec}_{i-1}^{t})-C(\tilde{\qvec}_{i}^{t})-(\tilde{\qvec}_{i-1}^{t}-\tilde{\qvec}_{i}^{t})^{T}\Phi(x^{t})]\\
%&=& E_{\qvec}[K(\pi||\tilde{\qvec}_{i-1}^{t})] + H(\pi) - E[K(\pi||\tilde{\qvec}_{i}^{t}) + H(\pi)]]
%\end{eqnarray*}
%Thus, it is sufficient to prove that E_{\qvec}[K(\pi||\tilde{\qvec}_{i-1}^{t}] >= E_{\qvec}[K(\pi||\tilde{\qvec}_{i}^{t}].

%In the case that 

%It is well known that minimizing the relative entropy of a predictive distribution with respect to the true distribution is equivalent to maximizing the likelihood function. % (see, for instance, \cite{whom?}). 
%In other words, the MLE of the natural parameters of an exponential family distribution minimizes the relative entropy of a predictive distribution with respect to the true distribution. 

%Further, for a true distribution $\pi$ and predictive distribution determined by $\qvec$, $K(\pi||\qvec) + H(\pi)$ is a convex function of $\qvec$. In fact, it is strictly convex when the representation is minimal. Thus, it has a unique minimum. %Further, $K(\pi||\rho) + H(\pi)$ is non-negative.

%Suppose the current market state is represented by $\tilde{\qvec}_{i-1}$ and $\qvec_{i}$ represents the belief of trader $i$. Since the trader is budget limited, he moves the market state to $\tilde{\qvec}_{i}$ instead where $\tilde{\qvec}_{i}=  \lambda \qvec_{i}+ (1-\lambda)\tilde{\qvec}_{i-1}$. For an informative trader $i$, $\qvec_{i}$ represents the MLE of the natural parameters of the distribution. Thus, $K(\pi||\qvec) + H(\pi)$ has a unique minimum at $\qvec_{i}$.

%Thus,   from Lemma \ref{lem:convex}, $E[K(\pi||\tilde{\qvec}_{i-1}^{t})+ H(\pi)] > E[K(\pi||\tilde{\qvec}_{i}^{t})+ H(\pi)]$ and trader $i$'s expected budget increases.
%\end{proof}

For continuous distributions with a density, the probability that a trader with private information will form exactly the same beliefs as the current market position is $0$, and thus, each trader will have positive expected profit on almost all sequences of observed samples and beliefs. 
This result suggests that, eventually, every informative trader will have the ability to influence the market state in accordance with his beliefs, without being budget limited.

We note one important aspect of Theorem~\ref{thm:growth}: The expectation is taken with respect to each trader's belief at the time of trade, rather than with respect to the true distribution. This is needed because we have made no assumptions about the optimality of the traders' belief updating procedure; depending on the distribution family and the prior distribution over parameter values, maximum likelihood estimation might not optimize the true expected score. If we assume that the traders' belief formation is optimal, then this growth result will extend to the true distribution as well.
%To summarize our results for this in a market with budget-limited traders, the total loss induced by malicious traders is limited, while the budget of  informative traders grows.
\section{The MaxEnt Market Mechanism: A Bayesian View}
\subsubsection{Conjugate Priors}

%In the previous section it's assumed that the agent has a belief $\q$, and moves the market state from initial state $\q_{init}$ to a convex combination $\tilde{\q} = \lambda \q + (1-\lambda)\q_{init}$. This note tries to justify this based on Bayesian updating, rather than budget limitations. (But it doesn't quite succeed.)

\medskip\noindent
Let $p(x;\theta)$ denote a probability density drawn from an exponential family with sufficient statistic $\phi : \mX \rightarrow \bR^d$, where $\theta$ is the natural parameter:
$$
p(x;\theta) = \exp \left[ \la \theta, \phi(x) \ra \right],
$$
and
$$
g(\theta) = \log \int_{\mX} \exp\la\phi(x),\theta\ra d\!x.
$$
Recall that $\grad g(\theta) = \Exp[\phi(x)]$ and $\grad^2 g(\theta) = \Var[\phi(x)]$. The family of conjugate priors is also an exponential family and takes the form
$$
p(\theta; n,\nu) = \exp\left[ \la n\nu,\theta \ra - ng(\theta) - h(\nu,n) \right].
$$
Here the feature map is $\psi(\theta) = (\theta, -g(\theta))$, the natural parameter is $(n\nu, n)$ where $n \in \bR$ and $\nu \in \bR^d$. The normalizer $h(\nu, n)$ is convex in $(n\nu,n)$. It is helpful to think of the prior as being based on a `phantom' sample of size $n$ and mean $\nu$. The justification for this is that
$$
\Exp_{\theta} \left[ \Exp_x \left[ \phi(x) | \theta \right]\right] 
= \Exp_{\theta} \left[\grad g(\theta)\right]
= \nu.
$$
%(The proof is straightforward but not obvious. I have a reference for this but it's impossible to read---it's a mathematical statistics paper.)

Suppose we draw a sample $X = (x_1,\ldots,x_m)$ of size $m$, and denote the empirical mean by $\mu[X] = \sum_{i=1}^m \phi(x_i)$. The posterior distribution is then
$$
p(\theta|X) \propto p(X|\theta)p(\theta|n,\nu) \propto \exp\left[ \la\mu[X]+n\nu,\theta\ra - (m+n)g(\theta) \right],
$$
and so the posterior mean is 
\begin{equation} \label{posterior-mean}
\frac{m\mu[X] + n\nu}{m+n}.
\end{equation}
Thus the posterior mean is a convex combination of the prior and posterior means, and their relative weights depend on the phantom and empirical sample sizes.

In the context of prediction markets, one could imagine an agent who uses the current market estimate as its prior, and draws a empirical sample of size $m$. Its belief then takes the form~(\ref{posterior-mean}), where $n$ depends on the importance the agent places on the market estimate (perhaps based on how long the market has been running). However, note that the \emph{mean parameter} becomes a convex combination of market state and empirical belief, and this does not translate to the \emph{natural parameter}, which is what we would have liked. The new natural parameter is
$$
\grad g^{-1} \left(\frac{m\mu[X] + n\nu}{m+n}\right) = \grad g^* \left(\frac{m\mu[X] + n\nu}{m+n}\right).
$$
where $g^*$ is the convex conjugate of $g$.

\subsubsection{Exponential Utility}

Assume the agent's belief distribution $p$ belongs to an exponential family, so it takes the form
%
\[
p(x;\theta) = \exp[\theta x - T(\theta)]
\]
%
where $\theta$ is the natural parameter and $T$ is the log-partition function. (I'm assuming that the sufficient statistic is $\phi(x) = x$ just for simplicity.) Assume also that the agent has an exponential utility for money $w$:
%
\[
U(w) = -\frac{1}{a} \exp(-aw).
\]
%
Here $a$ is the coefficient of risk aversion (higher means more risk averse, and the utility function is more concave).

%
\begin{proposition}
An agent with exponential family belief with natural parameter $\htheta$, and exponential utility with coefficient $a$, makes a trade that moves the current market share vector $\theta$ to the convex combination $\frac{1}{1+a} \htheta + \frac{a}{1+a} \theta$ assuming an LMSR cost function.
\end{proposition}
%
%
\begin{proof}
Let $\delta$ be the vector of shares the agent trades. The payoff given eventual outcome $x$ is then $\delta x - C(\delta + \theta) + C(\theta)$. The utility for this payoff is as follows (recall that $\htheta$ is the agent's believed natural parameter).
%
\[ U( \delta x - C(\delta + \theta) + C(\theta) ) = -\frac{1}{a} \exp(-a \delta x + a C(\delta + \theta) - a C(\theta)).
\]
%
Taking the expected utility, we obtain
%
\begin{eqnarray*}
&   & \Exp\left[ U( \delta x - C(\delta + \theta) + C(\theta) ) \right] \\
& = & \int_{\mX} -\frac{1}{a} \exp(-a \delta x + a C(\delta + \theta) - a C(\theta)) \exp[\htheta x - T(\htheta)]\, dx \\
& = & -\frac{1}{a} \int_{\mX} \exp[(\htheta -a \delta) x + a C(\delta + \theta) - a C(\theta)) - T(\htheta)]\, dx \\
& = & -\frac{1}{a} \exp[a C(\delta + \theta) - a C(\theta)) + T(\htheta - a\delta) - T(\htheta)] \int_{\mX} \exp[(\htheta -a \delta) x - T(\htheta -a \delta)]\, dx \\
& = & -\frac{1}{a} \exp[a C(\delta + \theta) - a C(\theta)) + T(\htheta - a\delta) - T(\htheta)] \int_{\mX} p(x; \htheta -a \delta)\, dx \\
& = & -\frac{1}{a} \exp[a C(\delta + \theta) - a C(\theta)) + T(\htheta - a\delta) - T(\htheta)]\\
& = & U\left(- C(\delta + \theta) + C(\theta)) - \frac{1}{a} T(\htheta - a\delta) + \frac{1}{a} T(\htheta)\right)
\end{eqnarray*}
%
The second-last equality follows from the fact that $\int_{\mX} p(x; \htheta -a \delta)\, dx = 1$. Since utility $U$ is monotone increasing, it is maximized by maximizing its argument, which is a concave function of $\delta$ by convexity of $C$ and $T$. The optimality condition for the argument is
%
\begin{equation} \label{eq:optim}
\grad C(\delta^* + \theta) = \grad T(\htheta - a\delta^*)
\end{equation}
%
Now if the market maker is using LMSR, then $C$ is the log-partition function of the corresponding exponential family and $C = T$. Then~(\ref{eq:optim}) can be solved by equating the arguments. This leads to $\delta^* = (\htheta - \theta) / (1+a)$, which moves the share vector to $\theta + \delta^* = \frac{1}{1+a} \htheta + \frac{a}{1+a} \theta$.
\end{proof}
%

In the statement of the result I use the term ``LMSR cost function'' somewhat loosely, because we are not necessarily dealing with a market over exhaustive, mutually exclusive outcomes. What is meant is the cost function that arises by taking the dual to entropy of the maxent distribution with given mean parameter $\mu$. As we've discussed this seems like the right generalization of LMSR to arbitrary mean parameter spaces. 

Note that as $a \rightarrow 0$, we approach risk neutrality and the agent moves the share vector all the way to its private estimate $\htheta$. As $a$ grows (agent grows more risk averse) the agent makes smaller and smaller trades that keep it closer to the current estimate $\theta$.

So there is a kind of congruence between exponential family beliefs and exponential utility (as the names would suggest). 
\subsection{Properties of the Maximum Entropy Market}
Consider the dual of the cost function, $C^{*}(\mu)$ defined as
$$C^{*}(\mu) = \sup_{\beta} \beta\cdot\mu - C(\beta)$$
This supremum is obtained at the value of $\beta$ for which $\mu = \nabla C(\beta)$; that is the natural parameter $\beta$ for which $\mu=\E_{p_{\beta}}[\phi(x)]$ is the mean parameter. Rewriting, 
$$C^{*}(\E_{p_{\beta}}[\phi(x)]) = \beta\cdot\E_{p_{\beta}}[\phi(x)] - C(\beta)$$
Thus, the mean parameters are the dual variables to the natural parameters.


\begin{thm} The value of the dual of the cost function is the negative entropy of the exponential family distribution obtained from backward mapping the mean parameters. \end{thm}
\proof
To see this, we note that for $p(x)=\exp\{\beta\cdot\phi(x)-C(\beta)\}$
\begin{eqnarray*}
-H(p) &=& \int_{x}p(x)\log p(x)\\
&=& \int_{x}p(x)[\beta\cdot\phi(x)-C(\beta)]\\
&=& \beta\cdot\int_{x}p(x)[\phi(x)]-\int_{x}p(x)C(\beta)\\
&=& \beta\cdot\E_{p}[\phi(x)]-C(\beta)
\end{eqnarray*}
\unproof
\rmk This result shows a nice parallel to LMSR, since the dual of the cost function of the LMSR is the negative entropy.\unrmk
\rmk The LMSR is essentially a special case applied to a multinomial distribution. The LMSR is known to have bounded ($\log n$) market maker loss. Thus, while in general the exponential family LMSR does not guarantee bounded market maker loss, for some special cases it can. 
\unrmk

{\thm Negative differential entropy $-H(p)\defeq \int_{x}p(x) \log p(x) dx$ is unbounded from above for an exponential family distribution.}
\proof
Note that, for an exponential family distribution
\begin{eqnarray*}
H(p) &=& -\int_{x}p(x)\log p(x)dx\\
&=& -\int_{x}p(x)[\beta\cdot \phi(x)-\psi(\beta)] dx\\
&=& -\beta\cdot\E[ \phi(x)]+\psi(\beta)
\end{eqnarray*}
If the range of $\beta$ is unbounded, the negative differential entropy is unbounded as well.
\unproof

%{\cor The exponential family market maker has unbounded worst case loss.}
%\proof From \cite{ACV13} we know that the worst case market maker loss can be expressed in terms of the conjugate of the cost function as 
%$$\sup_{\mu\in\phi(\mathcal{X})}C^{*}(\mu) - \min_{\mu\in\conv(\phi(\mathcal{X}))}C^{*}(\mu)$$ 
%Unbounded differential entropy for an exponential family distribution thus implies unbounded market maker loss.\unproof
\paragraph{Loss of the market maker}
First we derive an expression for the conjugate dual $C^{*}$ in terms of the primal variables.

Recall the definition of the conjugate dual:
$$C^{*}(\mu) = \sup_{q}q\cdot\mu - C(q)$$
The supremum is achieved at $q'$ such that $\nabla C(q')=\mu$. So we may rewrite:
$$C^{*}(\nabla C(q')) = \sup_{q}q\cdot\nabla C(q') - C(q)$$
This supremum is achieved at $q$ such that $\nabla C(q')=\nabla C(q)$. One such value of $q$ is $q'$.
So we have 
$$C^{*}(\nabla C(q')) = q'\cdot\nabla C(q') - C(q')$$
Also, recall that $\nabla C^{*}(\nabla C(q))=q$ for the exponential family market. (I think there are some restrictions on $q$.)

Let $q_{0}$ be the initial and $q_{f}$ the final market state. Then if $\mu$ is the expected value of the outcome sufficient statistics (\ie  the mean parameter) under the true distribution, the loss of the exponential family market maker can be written as:
\begin{eqnarray*}
\phi(x)(q_{f}-q_{0})-C(q_{f})+C(q_{0})&=& \phi(x) (q_{f}-q_{0})-q_{f}\nabla C(q_{f})+ C^{*}(\nabla C(q_{f}))\\
&&+q_{0}\nabla C(q_{0})- C^{*}(\nabla C(q_{0}))\\
&=&q_{f}(\phi(x) -\nabla C(q_{f}))+ C^{*}(\nabla C(q_{f}))\\
&&-q_{0}(\phi(x) -\nabla C(q_{0}))- C^{*}(\nabla C(q_{0}))\\
&=&-C^{*}(\nabla C(q_{0}))+ C^{*}(\phi(x))-q_{0}(\phi(x) -\nabla C(q_{0}))\\
&&+C^{*}(\nabla C(q_{f}))- C^{*}(\phi(x))+q_{f}(\phi(x) -\nabla C(q_{f}))\\
&=&  C^{*}(\phi(x))-C^{*}(\nabla C(q_{0}))-\nabla C^{*}(\nabla C(q_{0}))(\phi(x) -\nabla C(q_{0}))\\
&&-\left[C^{*}(\phi(x))-C^{*}(\nabla C(q_{f}))-\nabla C^{*}(\nabla C(q_{f}))(\phi(x) -\nabla C(q_{f}))\right]\\
&=&D_{C^{*}}[\phi(x),\nabla C(q_{0}))]-D_{C^{*}}[\phi(x),\nabla C(q_{f}))]
\end{eqnarray*}

\subsubsection{Example: Gaussian Markets}
{\ex We will now derive an expression for the dual of the log partition function for the Gaussian distribution. In this case,  $\beta$ is a $2$-dimensional  vector. Let $\beta=\begin{pmatrix}\beta_{1}\\ \beta_{2}\end{pmatrix}$ and $\mu=\begin{pmatrix}\mu_{1}\\ \mu_{2}\end{pmatrix}$. So 
\begin{eqnarray*}
\pmb{\mu}&=&\nabla\psi(\pmb{\beta})\\
&=&\nabla\left(-\frac{\beta_{1}^{2}}{4\beta_{2}}-\frac{1}{2}\log(-2\beta_{2})\right)\\
&=&\begin{pmatrix}-\frac{\beta_{1}}{2\beta_{2}}\\ \frac{\beta_{1}^{2}}{4\beta_{2}^{2}}-\frac{1}{2\beta_{2}}\end{pmatrix}
\end{eqnarray*}
Thus, $\pmb{\beta}$ can be written in terms of $\pmb{\mu}$ as follows:
$\beta_{1}=\frac{\mu_{1}}{\mu_{2}-\mu_{1}^{2}}$ and $\beta_{2}=\frac{1}{\mu_{1}^{2}-\mu_{2}}$. This leads to the following closed form expression for the dual of the log partition function:
$$\psi^{*}(\mu)=-\frac{1}{2}-\frac{1}{2}\log(\mu_{2}-\mu_{1}^{2})$$}
{\ex
We will now derive the expression for the differential entropy for the normal distribution. Recall that for the normal distribution $$p(x) = \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left\{\frac{-(x-\mu)^{2}}{2\sigma^{2}}\right\}$$
\begin{eqnarray*}
H(p) &=& -\int_{x}p(x)\log p(x)dx\\
&=& -\int_{x}p(x)\left[-\log(2\pi)-\log\sigma+\frac{-(x-\mu)^{2}}{2\sigma^{2}}\right]dx\\
&=& \log(2\pi)+\log\sigma+\frac{1}{2\sigma^{2}}\int_{x}p(x)(x-\mu)^{2}]dx\\
&=& \log(2\pi)+\log\sigma+\frac{1}{2\sigma^{2}}\E_{p(x)}[(x-\mu)^{2}]dx\\
&=& \log(2\pi)+\log\sigma+\frac{1}{2\sigma^{2}}\sigma^{2}dx\\
&=& \log(2\pi)+\frac{1}{2}+\log\sigma 
\end{eqnarray*}
Recall that the value of the dual of the log partition function is the value of the negative entropy of the distribution for an exponential family distribution. Thus when the variance of the normal distribution $\sigma^{2} = 0$, the value of the dual of the cost function goes to $\infty$ and is thus unbounded.}

{\ex For the normal distribution, the natural parameters $\pmb{\beta}=(\beta_{1},\beta_{2})$ can be written in terms of $\mu$ and $\sigma$ as $$\beta_{1}=\frac{\mu}{\sigma^{2}},\quad \beta_{2}=\frac{-1}{2\sigma^{2}}$$ and the log partition function is  $$\psi(\pmb{\beta})=-\frac{\beta_{1}^{2}}{4\beta_{2}}-\frac{1}{2}\log(-2\beta_{2})$$
Alternately, the log partition function can be written in terms of its variance and mean as $\frac{\mu^{2}}{2\sigma^{2}}+\log\sigma$.
Also note that the mean parameters are $\mu$ and $\mu^{2}+\sigma^{2}$.}
Let's now work out the loss for the Gaussian market maker. First note that for a Gaussian market for mean parameters $\begin{pmatrix}\mu_{1}\\ \mu_{1}^{2}+\sigma_{1}^{2}\end{pmatrix}$ and $\begin{pmatrix}\mu_{2}\\ \mu_{2}^{2}+\sigma_{2}^{2}\end{pmatrix}$, we have 
\begin{eqnarray*}
D_{C^{*}}\left(\begin{pmatrix}\mu_{1}\\ \mu_{1}^{2}+\sigma_{1}^{2}\end{pmatrix},\begin{pmatrix}\mu_{2}\\ \mu_{2}^{2}+\sigma_{2}^{2}\end{pmatrix}\right)
&=& C^{*}\left(\begin{pmatrix}\mu_{1}\\ \mu_{1}^{2}+\sigma_{1}^{2}\end{pmatrix}\right)-C^{*}\left(\begin{pmatrix}\mu_{1}\\ \mu_{1}^{2}+\sigma_{1}^{2}\end{pmatrix}\right)\\
&&-\nabla C^{*} \left(\begin{pmatrix}\mu_{2}\\ \mu_{2}^{2}+\sigma_{2}^{2}\end{pmatrix}\right)\begin{pmatrix}\mu_{1}-\mu_{2}\\ \mu_{1}^{2}+\sigma_{1}^{2}-\mu_{2}^{2}-\sigma_{2}^{2}\end{pmatrix}\\
&=& -\log \frac{\sigma_{1}}{\sigma_{2}}-\begin{pmatrix}\frac{\mu_{2}}{\sigma_{2}^{2}}\\ -\frac{1}{2\sigma_{2}^{2}}\end{pmatrix}\cdot \begin{pmatrix}\mu_{1}-\mu_{2}\\ \mu_{1}^{2}+\sigma_{1}^{2}-\mu_{2}^{2}-\sigma_{2}^{2}\end{pmatrix}\\
&=& -\log \frac{\sigma_{1}}{\sigma_{2}}-\frac{\mu_{2}(\mu_{1}-\mu_{2})-\frac{1}{2}(\mu_{1}^{2}-\mu_{2}^{2}+\sigma_{1}^{2}-\sigma_{2}^{2})}{\sigma_{2}^{2}}\\
%&=& \log \frac{\sigma_{2}}{\sigma_{1}}-\frac{\sigma_{1}^{2}-\sigma_{2}^{2}-(\mu_{1}-\mu_{2})^{2}}{2\sigma_{2}^{2}}\\
&=& \log \frac{\sigma_{2}}{\sigma_{1}}+\frac{\sigma_{2}^{2}-\sigma_{1}^{2}}{2\sigma_{2}^{2}}+\frac{(\mu_{1}-\mu_{2})^{2}}{2\sigma_{2}^{2}}
\end{eqnarray*}

So the Gaussian market maker loss is 
%parallels to quadratic market maker loss? 
\begin{eqnarray*}
D_{C^{*}}[\phi(x),\nabla C(q_{0}))]-D_{C^{*}}[\phi(x),\nabla C(q_{f}))]&=& \log \frac{\sigma_{0}}{\sigma_{x}}+\frac{\sigma_{0}^{2}-\sigma_{x}^{2}}{2\sigma_{0}^{2}}+\frac{(\mu_{x}-\mu_{0})^{2}}{2\sigma_{0}^{2}}\\
&&-\left[\log \frac{\sigma_{f}}{\sigma_{x}}+\frac{\sigma_{f}^{2}-\sigma_{x}^{2}}{2\sigma_{f}^{2}}+\frac{(\mu_{x}-\mu_{f})^{2}}{2\sigma_{f}^{2}}\right]\\
&=& \log \frac{\sigma_{0}}{ \sigma_{f}}+\frac{(x-\mu_{0})^{2}}{2\sigma_{0}^{2}}-\frac{(x-\mu_{f})^{2}}{2\sigma_{f}^{2}}\\
\end{eqnarray*}
Here we have used the fact that $\mu_{x}=x$ and $\sigma_{x}=0$.
\rmk In statistics, the standard score (aka z-score), $(x-\mu)/\sigma$, is the (signed) number of standard deviations an observation is above the mean. \unrmk
\section{Generalized Log Scoring Rule}
Suppose that an agent holds a belief in the expected value of the moments of a random variable $x$. The maximum entropy distribution that is consistent with these beliefs is an exponential family distribution.
 \defn For an outcome $x$ with predicted probability $p$, the generalized log scoring rule is defined as $$S(p,x)\defeq a+b\log p(x)$$ where $a$ and $b$ are constants that we will set to $0$ and $1$ respectively in these notes.\undefn

\begin{thm} The generalized log scoring rule for the maximum entropy distribution is proper.
\end{thm}
\proof
Recall that a  proper scoring rule $\mathbf{S}$ satisfies $$ \E_{x\sim p}[\mathbf{S}(p,x)]\geq \E_{x\sim p}[\mathbf{S}(p^{\prime},x)]$$
 
Note that the maximum entropy distribution is an exponential family distribution. 
 Let $p(x)=\exp\{\beta\cdot \phi(x)-\psi(\beta)\}$ be the true distribution and  $p'(x)=\exp\{\beta'\cdot \phi(x)-\psi(\beta')\}$ be the predicted distribution on the event $x$. 
 
 We need to show that $$\E_{x\sim p}[\mathbf{S}(p,x)]\holds{\geq} \E_{x\sim p}[\mathbf{S}(p',x)]$$
  In other words, we need $$\E_{x\sim p}[\log p(x)]\holds{\geq} \E_{x\sim p}[\log p'(x)]$$
   Equivalently, $$\E_{x\sim p}[\beta\cdot \phi(x)-\psi(\beta)]\holds{\geq} \E_{x\sim p}[\beta'\cdot \phi(x)-\psi(\beta')]$$
   Or 
\begin{equation}\label{eq:proper}
\beta\cdot \E_{x\sim p}[\phi(x)]-\psi(\beta)\holds{\geq} \beta'\cdot \E_{x\sim p}[\phi(x)]-\psi(\beta')
\end{equation}

   For an exponential family distribution, $\E_{x\sim p}[\phi(x)]=\nabla\psi(\beta)$. Substituting in  $(\ref{eq:proper})$, we need to show
   $$\beta\cdot \nabla\psi(\beta)-\psi(\beta)\holds{\geq} \beta'\cdot \nabla\psi(\beta)-\psi(\beta')$$
   Rearranging the terms,
   $$(\beta-\beta')\cdot \nabla\psi(\beta)\holds{\geq} \psi(\beta)-\psi(\beta')$$   
   which is the first order condition on the convexity of $\psi(\cdot)$.
 \unproof   
 
 \rmk The distance between the scores of the true and any predictive exponential family distribution is, in fact,  the Bregman divergence (based on the log partition function) between the corresponding natural parameters. That is,
   $$\E_{x\sim p}[\mathbf{S}(p,x)]- \E_{x\sim p}[\mathbf{S}(p',x)] = D_{\psi}(\beta,\beta')$$
   \unrmk
 
 \subsection{Exponential Smoothing}
One way to model the exponential family prediction market might be through exponential smoothing.
\begin{itemize}
\item A \emph{time series} is a set of data points sampled at periodic instances.
\item \emph{Smoothing} is the process of creating an approximation function based on time series that while capturing  patterns in the data is relatively insensitive to noise.
\item Natural first step is to use moving average. But average on how many samples (say $k$)? What to do until the first $k$ samples have been received?
\item
Next attempt might be to use weighted moving average. Advantage is that you can give higher weight to more recent terms. But still same disadvantage as simple moving average technique.
\item
\emph{Exponential Smoothing} attempts to remove this disadvantage.
\end{itemize}

Exponential smoothing is defined iteratively for $\alpha\in(0,1)$ as
\begin{eqnarray*}
s_{1} &=& x_{0}\\
s_{t}&=&\alpha x_{t-1}+(1-\alpha)s_{t-1}
\end{eqnarray*}

Smaller values of $\alpha$ means greater smoothing e.g., $\alpha=0$ gives a constant function ($=x_{0}$).
Note that the selection of $x_{0}$ is important -- and increases in importance as $\alpha$. Some advantages over moving average: all data points matter (decreasing importance with time) and computationally only last data point need be stored.

\section{Bayesian Traders}
Suppose data is drawn from an exponential family distribution. Further suppose, following a Bayesian model, that each trader has some prior belief distribution over the parameters of the data distribution. Assuming a conjugate prior, this prior distribution is also an exponential family distribution.

The data distribution is given by $\exp\{\beta\cdot\phi(x)-\psi(\beta)\}$ where $\psi()$ is the log partition function and $\phi()$ are the sufficient statistics. Then the conjugate prior parametrized by $b_{0}=(n\nu, n)$ is given by $\exp\{b_{0}\cdot(\beta,\psi(\beta))-\Psi(b_{0})\}$where $\Psi()$ is the corresponding log partition function. The posterior distribution is given by $b=(n\nu+m\hmu, n+m)$ where $\hmu$ is the empirical mean of the sufficient statistics of the $m$ data points drawn from the data distribution, $\hmu=\sum_{i=1}^{m}\phi(x_{i})$. We also have $\E_{\beta\sim b_{0}}\E_{x\sim \beta}[\phi(x)]=\nu$.

\subsection{Linear Utility}

Suppose the trader wishes to maximize his expected payoff. Then the number of shares $\delta$ that he purchases when the current market state is $\theta$ is given by $$\arg\max_{\delta}\E_{\beta\sim b}\E_{x\sim \beta}\left[ \delta\phi(x) - C(\delta + \theta) + C(\theta)\right]$$
But, $\E_{\beta\sim b}\E_{x\sim \beta}[\phi(x)]=\frac{n\nu+m\hmu}{ n+m}$. To obtain the maximum, we set the gradient to $0$. Thus, we have $$\nabla C(\delta + \theta)=\frac{n\nu+m\hmu}{ n+m}$$ For an exponential family prediction market, the final market state is given by
$$\nabla C^{*}\left(\frac{n\nu+m\hmu}{ n+m}\right)$$

\subsection{Exponential Utility}
Now suppose the trader has exponential utility, given by $$U(w) = -\frac{1}{a} \exp(-aw)$$ where $a$ is the coefficient of risk aversion. Suppose the trader wishes to maximize his expected utility. Then the number of shares $\delta$ that he purchases when the current market state is $\theta$ is given by 
\begin{eqnarray*}
&&\arg\max_{\delta}\E_{\beta\sim b}\E_{x\sim \beta}U\left[ \delta\phi(x) - C(\delta + \theta) + C(\theta)\right]\\
&=&\arg\max_{\delta}\E_{\beta\sim b}\E_{x\sim \beta}-\frac{1}{a}\exp\left[ -a\delta\phi(x) +a C(\delta + \theta) -a C(\theta)\right]
\end{eqnarray*}

\begin{eqnarray*}
\E_{\beta\sim b}\E_{x\sim \beta}\exp\left[-a\delta\phi(x)\right]&=&\int_{\mathcal{B}}\int_{\mathcal{X}}\exp[-a\delta\phi(x)]\, \exp\{\beta\cdot\phi(x)-\psi(\beta)\}\, dx\\
&&\qquad\exp\{(n\nu+m\hmu)\cdot\beta-(n+m)\psi(\beta))-\Psi((n\nu+m\hmu, n+m))\}\, d\beta\\
&=&\int_{\mathcal{B}}\exp\{(n\nu+m\hmu)\cdot\beta-(n+m)\psi(\beta))-\Psi((n\nu+m\hmu, n+m))\}\\
&&\qquad\exp\{\psi(\beta-a\delta)-\psi(\beta)\}\,\, d\beta\int_{\mathcal{X}}\exp[(\beta-a\delta)\cdot\phi(x)-\psi(\beta-a\delta)]\, dx\\
&=&\int_{\mathcal{B}}\exp\{(n\nu+m\hmu)\cdot\beta-(n+m)\psi(\beta))-\Psi((n\nu+m\hmu, n+m))\}\\
&&\qquad\exp\{\psi(\beta-a\delta)-\psi(\beta)\}\,\, d\beta\\
\end{eqnarray*}

\section{Reinterpreting repeated trades in a market}
Let us suppose that traders in this market have exponential utility \[
U(w) = -\frac{1}{a} \exp(-aw).
\]
%
Here $a$ is the coefficient of risk aversion (higher means more risk averse, and the utility function is more concave).

Let $\delta_{1}^{*}$ be the optimal vector of shares the agent decides to trade on first entering the market with exponential family belief parametrized by natural parameter $\hat{\theta}$. Thus, his belief distribution is given by the pdf $$\exp\{\hat{\theta} \phi(x) - T(\hat{\theta})\}]$$ where $T(\hat{\theta}) =\int_{\mathcal{X}}\exp\{\hat{\theta} \phi(x)\}\ dx$ is the log partition function and $\phi(x)$ are the sufficient statistics.
On a subsequent entry into this market when the market state is $\theta'$, his optimal purchase $\delta_{2}^{*}$ is given by the solution of
$$\arg\max_{\delta_{2}} \E_{x\sim \hat{\theta}} U[\mathrm{payoff}]$$
Here, the payoff given eventual outcome $x$ is  $[\delta_{1}^{*}+\delta_{2}^{*} ]\phi(x) - C(\delta_{1}^{*} + \theta) + C(\theta)- C(\delta_{2}^{*} + \theta') + C(\theta')$. Here $\theta' = \theta+\delta_{1}^{*}+\delta'$ where $\delta'$ captures the movement of the share vector by other traders in the market. 
The expected utility for this payoff is as follows.
%%
%\[ U( \delta x - C(\delta + \theta) + C(\theta) ) = -\frac{1}{a} \exp(-a \delta x + a C(\delta + \theta) - a C(\theta)).
%\]
%%
\begin{eqnarray*}
&   & \arg\max_{\delta_{2}}\E\left[ (\delta_{1}^{*}+\delta_{2} )\phi(x) - C(\delta_{1}^{*} + \theta) + C(\theta)- C(\delta_{2} + \theta') + C(\theta') \right] \\
& = & \arg\max_{\delta_{2}}\int_{\mathcal{X}} -\frac{1}{a} [\exp(-a (\delta_{1}^{*}+\delta_{2} )\phi(x) \\
&  & + a C(\delta_{1}^{*} + \theta) -a C(\theta)+a C(\delta_{2} + \theta') -a C(\theta')) \exp\{\hat{\theta} \phi(x) - T(\hat{\theta})\}]\, dx \\
& = & \arg\max_{\delta_{2}}\int_{\mathcal{X}} -\frac{1}{a} [\exp\{-a \delta_{2} \phi(x)+a C(\delta_{2} + \theta') -a C(\theta') \\
&  & + a C(\delta_{1}^{*} + \theta) -a C(\theta)\} \exp\{(\hat{\theta}-a \delta_{1}^{*}) \phi(x) - T(\hat{\theta})\}]\, dx \\
& = & \arg\max_{\delta_{2}}\int_{\mathcal{X}} U[N(\delta_{2},\theta')] \exp\{(\hat{\theta}-a \delta_{1}^{*}) \phi(x) - T(\hat{\theta})+ a C(\delta_{1}^{*} + \theta) -a C(\theta)\}]\, dx 
\end{eqnarray*}

Here the first equality follows from the fact that we are taking expectation over the traders belief parameter $\hat{\theta}$ and the second equality follows simply from rearranging the factors of $\phi(x)$. And lastly we have written $-\frac{1}{a} [\exp\{-a \delta_{2} \phi(x)+a C(\delta_{2} + \theta') -a C(\theta')]$ as $U[N(\delta_{2},\theta')]$ where $N(\delta_{2},\theta')$ is the net payoff when $\delta_{2}$ shares are purchased when the current market state is $\theta'$. 

This is equivalent to maximizing expected utility of $N(\delta_{2},\theta')$, where expectation is taken with respect to an exponential family distribution over $\mathcal{X}$ parametrized by $\hat{\theta}-a \delta_{1}^{*}$, so long as 
$$\int_{\mathcal{X}} \exp\{(\hat{\theta}-a \delta_{1}^{*}) \phi(x) - T(\hat{\theta})+ a C(\delta_{1}^{*} + \theta) -a C(\theta)\}]\, dx = c\times \int_{\mathcal{X}} \exp\{(\hat{\theta}-a \delta_{1}^{*}) \phi(x)-T(\hat{\theta}-a \delta_{1}^{*})\} \, dx$$

%$$\int_{\mathcal{X}} \exp\{(\hat{\theta}-a \delta_{1}^{*}) \phi(x)-\psi(\hat{\theta}-a \delta_{1}^{*})\} \, dx =1$$
%That is
%$$\psi(\hat{\theta}-a \delta_{1}^{*})= \int_{\mathcal{X}} \exp\{(\hat{\theta}-a \delta_{1}^{*}) \phi(x)\}\ dx$$
In other words we want the following statement to hold.
\begin{eqnarray*}
\exists c: c\times\exp\{- T(\hat{\theta})+ a C(\delta_{1}^{*} + \theta) -a C(\theta)\}&=&
T(\hat{\theta}-a \delta_{1}^{*})\\
&=& \int_{\mathcal{X}} \exp\{(\hat{\theta}-a \delta_{1}^{*}) \phi(x)\}\ dx
\end{eqnarray*}

where $c$ is a constant with respect to $\delta_{2}$ and $x$ (but could potentially depend on $\hat{\theta}$, $a$ and $\delta_{1}^{*}$). This is true because the right hand side of the equation is a function of only $\hat{\theta}$, $a$ and $\delta_{1}^{*}$. Let $\Theta\defeq\hat{\theta}-a \delta_{1}^{*}  $ be the effective belief. Thus we have that the trader chooses his share vector as follows.
\begin{eqnarray*}
&&  \arg\max_{\delta_{2}}\int_{\mathcal{X}} U[N(\delta_{2},\theta')] \exp\{(\hat{\theta}-a \delta_{1}^{*}) \phi(x) - T(\hat{\theta})+ a C(\delta_{1}^{*} + \theta) -a C(\theta)\}\, dx \\
& = &\arg\max_{\delta_{2}}\int_{\mathcal{X}} U[N(\delta_{2},\theta')] \exp\{(\hat{\theta}-a \delta_{1}^{*}) \phi(x) - T(\hat{\theta}-a \delta_{1}^{*})\}]\, dx\\
& = & \arg\max_{\delta_{2}} \int_{\mathcal{X}} -\frac{1}{a} \exp\{-a \delta_{2} \phi(x)+a C(\delta_{2} + \theta') -a C(\theta')\} \exp\{\Theta\cdot \phi(x) - T(\Theta)\}\, dx\\
& = & \arg\max_{\delta_{2}} U\left[- C(\delta_{2} + \theta') + C(\theta') - \frac{1}{a} T(\Theta - a\delta_{2}) + \frac{1}{a} T(\Theta)\right]\\
\end{eqnarray*}
This follows from the argument detailed in in exp-util and simple substitution. Further,  the maximizer is $\delta_{2}^* = (\Theta - \theta') / (1+a)$, which moves the share vector to $\theta' + \delta_{2}^* = \frac{1}{1+a} \Theta + \frac{a}{1+a} \theta'$ which is a convex combination of the effective belief and the current market state.

In other words, an exponential utility maximizing trader who has belief $\hat{\theta}$ with prior exposure $\delta$ in a market  will behave identically as an exponential utility maximizing trader with belief $\hat{\theta}-a \delta$ and no prior exposure in the market. Here $a$ is the utility parameter. This means that financial exposure can be equivalently understood as changing the privately held beliefs.

Note that as the trader becomes more risk averse, the  belief is updated more aggressively. As $a \rightarrow 0$, the trader becomes more risk neutral and his effective belief is closer to his true belief. 
%
%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\subsection{Equilibrium in a market with multiple traders}
We have shown that every exponential-utility maximizing trader picks the share vector $\delta_{i}$ so as to maximize the utility of $- C(\delta_{i} + \theta) + C(\theta) - \frac{1}{a_{i}} T(\beta_{i} - a_{i}\delta_{i}) + \frac{1}{a_{i}} T(\beta_{i})$ where $\theta$ is the initial market state, $a_{i}$ the utility parameter and $\beta_{i}$ the belief parameter of  trader $i$. To maximize utility, it suffices to maximize the argument, assuming that $C()=T()=\psi()$. 

Now consider a market with multiple exponential-utility maximizing traders. We wish to compute the equilibrium state. Let us assume that equilibrium is reached at a market state $\theta+\sum_{i}\delta_{i}$ where $\theta$ is the initial market state before trading begins and $\delta_{i}$ is each trader's purchase. 

%Thus, each trader picks $\delta_{i}$ as follows.
%\begin{eqnarray*}
%&&\arg\max_{\delta_{i}}\left(- \psi(\theta+\sum_{i}\delta_{i}) + \psi(\theta) - \frac{1}{a_{i}} \psi(\beta_{i} - a_{i}\delta_{i}) + \frac{1}{a_{i}} \psi(\beta_{i})\right)\\
%&\iff&\arg\min_{\delta_{i}}\left( \psi(\theta+\sum_{i}\delta_{i})+\sum_{i}\frac{1}{a_{i}}\psi(\beta_{i}-a_{i}\delta_{i})\right)
%\end{eqnarray*}
Recall the following result from game theory.
\begin{thm}\label{thm:potl}
Let $U_{i}(\vec{\delta})$ be the utility function of the $i^{th}$ trader given strategies $\vec{\delta}\defeq \delta_{1},\ldots,\delta_{i},\ldots,\delta_{n}$. If there exists a potential function $\phi(\vec{\delta})$ such that $$U_{i}(\vec{\delta})-U_{i}(\vec{\delta}_{-i},\delta_{i}')=\phi(\vec{\delta})-\phi(\vec{\delta}_{-i},\delta_{i}')$$
then when $\phi(\vec{\delta})$ is maximized, $\vec{\delta}$ is an equilibrium.
\end{thm}

Let $$\phi(\vec{\delta})\defeq  \psi(\theta+\sum_{i}\delta_{i})+\sum_{i}\frac{1}{a_{i}}\psi(\beta_{i}-a_{i}\delta_{i})$$
Now $U_{i}(\vec{\delta})=- \psi(\theta+\sum_{j}\delta_{j}) + \psi(\theta+\sum_{j\neq i}\delta_{j}) - \frac{1}{a_{i}} \psi(\beta_{i} - a_{i}\delta_{i}) + \frac{1}{a_{i}} \psi(\beta_{i})$.
Thus, Theorem \ref{thm:potl} applies and we can find the equilibrium market state by maximizing $\phi(\vec{\delta})$ for each $\delta_{i}$.
\begin{eqnarray*}
\nabla_{\delta_{i}}\phi(\vec{\delta})&=&\nabla\psi(\theta+\sum_{j=1}^{n}\delta_{i})-\nabla\psi(\beta_{i}-a_{i}\delta_{i})\\
&=&0
\end{eqnarray*}
This can be achieved by equating the arguments. That is, for each trader $i$, 
\begin{equation}\label{eq:eqbm}
\beta_{i}-a_{i}\delta_{i}=\theta+\sum_{j=1}^{n}\delta_{j}
\end{equation}
Rewriting, we have for each trader $i$, 
$$\frac{\beta_{i}}{a_{i}}-\delta_{i}=\frac{1}{a_{i}}(\theta+\sum_{j=1}^{n}\delta_{j})$$
Thus,
$$\sum_{i=1}^{n}\left(\frac{\beta_{i}}{a_{i}}\right)-\sum_{i=1}^{n}\delta_{i}=\left(\theta+\sum_{j=1}^{n}\delta_{j}\right)\sum_{i=1}^{n}\frac{1}{a_{i}}$$
And
$$\sum_{j=1}^{n}\delta_{j}=
\frac{\sum_{i=1}^{n}\left(\frac{\beta_{i}}{a_{i}}\right)-\theta\sum_{i=1}^{n}\left(\frac{1}{a_{i}}\right)}{1+\sum_{i=1}^{n}\frac{1}{a_{i}}}$$
Substituting in Equation \ref{eq:eqbm} we have the following expression for the final market state.
$$\theta+\sum_{j=1}^{n}\delta_{j}=
\frac{\theta+\sum_{i=1}^{n}\left(\frac{\beta_{i}}{a_{i}}\right)}{1+\sum_{i=1}^{n}\frac{1}{a_{i}}}$$
Let us modify the cost function so that it includes a liquidity parameter:
$$C_{\lambda}(q)\defeq\lambda \psi(q/\lambda)$$

\subsubsection{Lower Bound on Expected Payoff for Exponential Utility Traders}
Let $\theta$ be the current market state. We have shown that an exponential utility trader with belief distribution  parametrized by $\beta$ will move the market state to $\theta'=\frac{1}{1+a} \beta + \frac{a}{1+a} \theta$. Therefore, the trader's expected net payoff is given by
\begin{eqnarray*}
&& \E_{x\sim P_{\beta}}[C(\theta)-C(\theta')-(\theta-\theta')\phi(x)]\\
&=& \psi(\theta)-\theta\E_{x\sim P_{\beta}}[\phi(x)]-(\psi(\theta')-\theta' \E_{x\sim P_{\beta}}[\phi(x)])\\
&=& \psi(\theta)-\theta\nabla\psi(\beta)-(\psi(\theta')-\theta' \nabla\psi(\beta))\\
&=& \psi(\theta)-\psi(\beta)-\nabla\psi(\beta)(\theta-\beta)-(\psi(\theta')-\psi(\beta)- \nabla\psi(\beta)(\theta'-\beta))\\
&=& D_{\psi}(\theta,\beta)-D_{\psi}(\theta',\beta)\\
&\geq&\frac{1}{a}D_{\psi}(\theta',\beta)\geq0
\end{eqnarray*}
The second to last inequality holds since $D_{\psi}(\theta',\beta)$ is convex in $\theta'$ and we have:
\begin{eqnarray*}
D_{\psi}(\theta',\beta)&=&D_{\psi}\left(\frac{1}{1+a} \beta + \frac{a}{1+a} \theta,\beta\right)\\
&\leq&\frac{1}{1+a}D_{\psi}( \beta ,\beta)+ \frac{a}{1+a} D_{\psi}( \theta ,\beta)\\
&=&\frac{a}{1+a} D_{\psi}( \theta ,\beta)
%\implies D_{\psi}(\theta',\beta)+aD_{\psi}(\theta',\beta)&\leq& aD_{\psi}( \theta ,\beta)\\
\end{eqnarray*}
Thus, a trader who moves the market state can expect his profit to be positive and at least $\frac{1}{a}D_{\psi}(\theta',\beta)$.
\section{Conclusions}

\end{document}


